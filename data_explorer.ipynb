{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f7cfa4-6898-4f51-8633-0fdd86f35977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_info import BiomTable, CSVTable\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import test_linear_clustering\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from plot_definitions import is_ms, is_new_sample_closure, build_new_samples_set, imsms_plots, imsms_qualitative_class\n",
    "from coexclusion import counts_to_presence_absence, counts_to_dynamic_presence_absence, presence_absence_to_contingency, pairwise_eval\n",
    "import os\n",
    "import biom\n",
    "from merge_similar_vectors import merge_similar_vectors\n",
    "from woltka_metadata import list_genera, list_woltka_refs\n",
    "\n",
    "\n",
    "from test_linear_clustering import recursive_subspacer, iterative_clustering, calc_projected, _fit_a_linear_subspace\n",
    "from plotting import _get_color, _draw_subspace_line, plot_taxavec3\n",
    "\n",
    "from linear_subspace_clustering import linear_subspace_clustering, calc_subspace_bases\n",
    "from cluster_manager import ClusterManager, ClusterState, RecursiveClusterer, OUTLIER_CLUSTER_ID\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import json\n",
    "import scipy\n",
    "import data_transform as transformer\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3d46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "WOLTKA_METADATA_PATH=\"./woltka_metadata.tsv\"\n",
    "DATA_TRANSFORM_PATH=\"./results/jupyter_interactive_foo.json\"\n",
    "\n",
    "ALL_SPECIES_VECS = \"./dataset/biom/species_vectors.biom\"\n",
    "SHARED_SPECIES_VECS = \"./dataset/biom/species_vectors_shared.biom\"\n",
    "AKKER_TABLE=\"./dataset/biom/akkermansia_foobar.biom\"\n",
    "\n",
    "BIOM_TABLE=\"./dataset/biom/imsms-combined-none.biom\"\n",
    "# BIOM_TABLE = [\n",
    "# #     Old Files\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-July24-2019-1of2.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-July24-2019-2of2.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-Aug7-2019.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-1of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-2of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-3of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-4of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-5of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-6of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-7of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-8of9.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-June6-2020-9of9.biom\",\n",
    "# #     New Files\n",
    "#     # \"./dataset/biom/imsms-woltka-none/imsms-none-Jul28-2022.biom\", # ARGH, This file is so low read count it should probably be tossed.\n",
    "#     # \"./dataset/biom/imsms-woltka-none/imsms-none-Aug1-2022.biom\", # ARGH, This file is so low read count it should probably be tossed.\n",
    "#     # \"./dataset/biom/imsms-woltka-none/imsms-none-Aug3-2022.biom\" # ARGH, This file is so low read count it should probably be tossed.\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-1of3.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-2of3.biom\",\n",
    "#     \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-3of3.biom\"\n",
    "# ]\n",
    "NEW_IMSMS_SAMPLES = [\n",
    "    \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-1of3.biom\",\n",
    "    \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-2of3.biom\",\n",
    "    \"./dataset/biom/imsms-woltka-none/imsms-none-Sep2-2022-3of3.biom\"\n",
    "]\n",
    "NEW_IMSMS_SAMPLES_SET = build_new_samples_set(NEW_IMSMS_SAMPLES)\n",
    "# BIOM_TABLE=\"./dataset/biom/finrisk-combined-none.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/sol_public_99006-none.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/bacteroides_isolates.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/staph_aureus.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/caitriona_matrix_tubes_none.biom\"\n",
    "# BIOM_TABLE=[\n",
    "#     \"./dataset/biom/Celeste_Prep_1_1428_samples.biom\",\n",
    "#     \"./dataset/biom/Celeste_Prep_2_672_samples.biom\",\n",
    "#     \"./dataset/biom/Celeste_Prep_3_936_samples.biom\",\n",
    "#     \"./dataset/biom/Celeste_Prep_4_792_samples.biom\"\n",
    "# ]\n",
    "# BIOM_TABLE=\"./dataset/biom/sahar_asd.biom\"\n",
    "\n",
    "# BIOM_TABLE = [\n",
    "#     \"./dataset/biom/zymo_mock_12201/140583_free.biom\",\n",
    "#     \"./dataset/biom/zymo_mock_12201/140570_free.biom\",\n",
    "#     \"./dataset/biom/zymo_mock_12201/140350_free.biom\",\n",
    "#     \"./dataset/biom/zymo_mock_12201/108098_free.biom\"\n",
    "# ]\n",
    "\n",
    "MIN_GENUS_COUNT = 500\n",
    "CONSTANT_SUM_SCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4272a2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3327, 4911)\n",
      "Q.71401.0009.2016.02.23     587718.0\n",
      "Q.71401.0049.2016.10.05     406728.0\n",
      "Q.71401.0088.2017.01.30    3992846.0\n",
      "Q.71401.0108.2017.05.16     507937.0\n",
      "Q.71401.0108.2017.05.19    1682393.0\n",
      "                             ...    \n",
      "S.71801.0030.9.29.16        949121.0\n",
      "S.71801.0038.11.30.16       303578.0\n",
      "S.71801.0057.2.3.17         687985.0\n",
      "S.71802.0038.11.29.16       243103.0\n",
      "S.71802.0067.3.8.17.rep    2234760.0\n",
      "Length: 3327, dtype: float64\n",
      "455341.0\n"
     ]
    }
   ],
   "source": [
    "woltka_meta_table = CSVTable(WOLTKA_METADATA_PATH, delimiter=\"\\t\")\n",
    "woltka_meta_df = woltka_meta_table.load_dataframe()\n",
    "\n",
    "if not isinstance(BIOM_TABLE, list):\n",
    "    BIOM_TABLE = [BIOM_TABLE]\n",
    "\n",
    "all_dfs = []\n",
    "for biom_table in BIOM_TABLE:\n",
    "    bt = BiomTable(biom_table)\n",
    "    df = bt.load_dataframe()\n",
    "    print(df.shape)\n",
    "    print(df.sum(axis=1))\n",
    "    print(df.sum(axis=1).median())\n",
    "    all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs).fillna(0)\n",
    "\n",
    "# print(df.sum())\n",
    "# all_df = None\n",
    "all_species_vecs = BiomTable(ALL_SPECIES_VECS)\n",
    "all_df = all_species_vecs.load_dataframe()\n",
    "\n",
    "all_df.loc[\"intestini\",:] = (all_df.loc[\"G000431295\",:] + all_df.loc[\"G000230275\",:]) / 2\n",
    "all_df.loc[\"fermentans\",:] = (all_df.loc[\"G000025305\",:] + all_df.loc[\"G900107075\",:] + all_df.loc[\"G900115425\",:]) / 3\n",
    "all_df.loc[\"CAG:196\",:] = (all_df.loc[\"G000433235\",:] + all_df.loc[\"G001917235\",:]) / 2\n",
    "\n",
    "shared_species_vecs = BiomTable(SHARED_SPECIES_VECS)\n",
    "shared_df = shared_species_vecs.load_dataframe()\n",
    "\n",
    "MS_TARGET = None\n",
    "if \"imsms\" in BIOM_TABLE[0]:\n",
    "    df['target'] = df.index.map(is_new_sample_closure(NEW_IMSMS_SAMPLES_SET))\n",
    "    # df['target'] = df.index.map(is_ms)\n",
    "    MS_TARGET = df['target']\n",
    "    MS_TARGET = MS_TARGET[~MS_TARGET.index.duplicated(keep='first')]\n",
    "    df = df.drop(['target'], axis=1)\n",
    "\n",
    "# if \"zymo\" in BIOM_TABLE[0]:\n",
    "#     zymo_meta = CSVTable(\"./dataset/biom/12201_20220510-095201.txt\", sep='\\t', index_col=\"sample_name\").load_dataframe()\n",
    "#     print(zymo_meta[\"sample_type_2\"].value_counts())\n",
    "#     zymo_samples = zymo_meta.index[zymo_meta[\"sample_type_2\"] == \"ZymoBIOMICS Microbial Community Standard I\"]\n",
    "#     # Restrict to zymo samples\n",
    "#     print(\"Restricting to zymo samples\")\n",
    "#     print(\"initial shape\", df.shape)\n",
    "#     df[\"sample_type_2\"] = zymo_meta[\"sample_type_2\"]\n",
    "#     print(df[\"sample_type_2\"])\n",
    "#     print(df[\"sample_type_2\"].value_counts())\n",
    "#     print((df[\"sample_type_2\"] == \"ZymoBIOMICS Microbial Community Standard I\").sum())\n",
    "#     df = df[df[\"sample_type_2\"] == \"ZymoBIOMICS Microbial Community Standard I\"]\n",
    "#     df = df.drop([\"sample_type_2\"], axis=1)\n",
    "#     print(\"final shape\", df.shape)\n",
    "    \n",
    "#     print(zymo_samples)\n",
    "#     print(df.index)\n",
    "    \n",
    "    \n",
    "#     # Run a basic abundance filter\n",
    "#     total_bugs = df.sum().sum()\n",
    "#     df = df[df.columns[df.sum() > total_bugs / 10000]]\n",
    "    \n",
    "    # Rescale columns (This is probably very stupid, but might give VRNMF a chance to fit to lower abundance columns)\n",
    "    # normalized_df=(df-df.min())/(df.max()-df.min())\n",
    "    # normalized_df=(df)/(df.max()) * 10000\n",
    "    # df = normalized_df.fillna(0)\n",
    "try:\n",
    "    with open(DATA_TRANSFORM_PATH) as infile:\n",
    "        data_transform = json.load(infile)\n",
    "except:\n",
    "    print(\"Couldn't open data transform, starting new data transform\")\n",
    "    data_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbd1e1-e5dc-4cdd-8f76-e93e3578fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a5d787-fd3b-47ee-979f-13fb2031ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.sum(axis=1)\n",
    "cst = df.divide(df_sum, axis='rows') * 10000\n",
    "\n",
    "if CONSTANT_SUM_SCALE:\n",
    "    df = cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b54ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17190675-fdc6-419f-94aa-2529797859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter3(filtered_df, cluster_manager, title, c1, c2, c3, target_cluster=None, subplot_ax=None, conic_bases=None):    \n",
    "    filtered_df = filtered_df.copy()\n",
    "    cs = None\n",
    "    cluster_counts = None\n",
    "    if cluster_manager is not None:\n",
    "        cs = cluster_manager.cluster_state\n",
    "        cluster_counts = cs.get_cluster_counts()\n",
    "    \n",
    "    # print(\"Num Clusters:\", cs.num_clusters())\n",
    "    if cs is None:\n",
    "        subspace_bases = None\n",
    "    elif cs.num_clusters() > 0:\n",
    "        subspace_bases = cluster_manager.calc_subspace_bases()\n",
    "        subspace_bases = {x: pd.DataFrame(subspace_bases[x], index=filtered_df.columns) for x in subspace_bases}\n",
    "        all_proj = calc_projected(filtered_df, cs.clusters, subspace_bases)\n",
    "    # elif state['genus'] in data_transform:\n",
    "    #     subspace_bases = data_transform[state['genus']]\n",
    "    #     subspace_bases = {i: pd.read_json(subspace_bases[i]) for i in range(len(subspace_bases))}\n",
    "    #     if len(subspace_bases) == 0:\n",
    "    #         subspace_bases = None\n",
    "    #     # print(\"Loaded bases:\", subspace_bases)\n",
    "    else:\n",
    "        subspace_bases = None\n",
    "    \n",
    "    if conic_bases is not None:\n",
    "        conic_bases = {x: pd.DataFrame(conic_bases[x], index=filtered_df.columns) for x in conic_bases}\n",
    "    \n",
    "    if subplot_ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "    else:\n",
    "        ax = subplot_ax\n",
    "\n",
    "    maxx = max(filtered_df[c1])\n",
    "    maxy = max(filtered_df[c2])\n",
    "    maxz = max(filtered_df[c3])\n",
    "    \n",
    "    col1_index = filtered_df.columns.get_loc(c1)\n",
    "    col2_index = filtered_df.columns.get_loc(c2)\n",
    "    col3_index = filtered_df.columns.get_loc(c3)\n",
    "\n",
    "    if target_cluster is not None:\n",
    "        pidx = np.where(cs.clusters == target_cluster)[0]\n",
    "        cdf = filtered_df.iloc[pidx]\n",
    "        maxx = max(cdf[c1])\n",
    "        maxy = max(cdf[c2])\n",
    "        maxz = max(cdf[c3])\n",
    "\n",
    "    if subspace_bases is not None:\n",
    "        min_label = -1\n",
    "        if -1 not in cluster_counts or cluster_counts[-1] == 0:\n",
    "            min_label = 0\n",
    "        max_label = max(subspace_bases)\n",
    "\n",
    "        x = np.linspace(0, maxx, 10)\n",
    "        y = np.linspace(0, maxy, 10)\n",
    "\n",
    "        if conic_bases is not None:\n",
    "            to_draw = conic_bases\n",
    "            if target_cluster is not None:\n",
    "                to_draw = [target_cluster]\n",
    "                \n",
    "            for label in to_draw:\n",
    "                dim = conic_bases[label].shape[1]\n",
    "                for u_idx in range(dim):\n",
    "                    u = conic_bases[label].iloc[:,u_idx].loc[[c1,c2,c3]]\n",
    "                    _draw_subspace_line(ax, u, maxx, maxy, maxz, label, min_label, max_label)\n",
    "        for label in subspace_bases:\n",
    "            dim = subspace_bases[label].shape[1]\n",
    "            if dim == 1 or dim > 2:\n",
    "                # unclear how to draw 3+ d surfaces, so just draw primary axis\n",
    "                u = subspace_bases[label].iloc[:,0].loc[[c1,c2,c3]]\n",
    "                _draw_subspace_line(ax,u,maxx,maxy,maxz,label,min_label,max_label)\n",
    "            if dim == 2:\n",
    "                # the basis vectors are orthogonal\n",
    "                # But when you project them down to this space, their projections may not be orthogonal\n",
    "                # They could even be parallel, or even 0 (Take u,v = 0,0,1,-1 and 0,0,1,1 projected to first three dimensions)\n",
    "                # If they are parallel, we should be drawing a line, not a plane\n",
    "                # If they are 0, we should just draw a single point (happens to be at the origin\n",
    "                u = subspace_bases[label].iloc[:,0].loc[[c1,c2,c3]]\n",
    "                v = subspace_bases[label].iloc[:,1].loc[[c1,c2,c3]]\n",
    "                _draw_subspace_line(ax,u,maxx,maxy,maxz,label,min_label,max_label)\n",
    "                _draw_subspace_line(ax,v,maxx,maxy,maxz,label,min_label,max_label)\n",
    "\n",
    "                npu = np.array(u)\n",
    "                npv = np.array(v)\n",
    "                lenu = np.linalg.norm(u)\n",
    "                lenv = np.linalg.norm(v)\n",
    "                if lenu < 0.01 and lenv < 0.01:\n",
    "                    # Both of the vectors are basically 0.  Can't draw anything.\n",
    "                    continue\n",
    "                elif lenu < 0.01:\n",
    "                    _draw_subspace_line(ax, v, maxx,maxy,maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "                elif lenv < 0.01:\n",
    "                    _draw_subspace_line(ax, u, maxx,maxy,maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "\n",
    "                npu = npu / lenu\n",
    "                npv = npv / lenv\n",
    "\n",
    "                dotproduct = np.dot(npu,npv)\n",
    "                if dotproduct > 0.9 or dotproduct < -0.9:\n",
    "                    print(dotproduct)\n",
    "                    # Vectors are basically parallel.  Can't draw a plane, maybe can draw a line.\n",
    "                    if dotproduct < 0:\n",
    "                        npu = -npu\n",
    "                    npu = (npu + npv) / 2\n",
    "                    _draw_subspace_line(ax, npu, maxx, maxy, maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "\n",
    "                normal = np.cross(u.T, v.T)\n",
    "                # Normal vector gives x*n0 + y*n1 + z*n2 = 0\n",
    "                # Divide everything by n2\n",
    "                # x * n0/n2 + y * n1/n2 + z = 0\n",
    "                # z = -n0/n2 * x - n1 / n2 * y\n",
    "                X, Y = np.meshgrid(x,y)\n",
    "                Z = -normal[0]/normal[2] * X - normal[1] / normal[2] * Y\n",
    "                rgba = _get_color(label, min_label, max_label)\n",
    "                rgba = rgba[0],rgba[1],rgba[2],0.25\n",
    "                surf = ax.plot_surface(X, Y, Z, color=rgba)\n",
    "\n",
    "\n",
    "    if target_cluster is None:\n",
    "        if MS_TARGET is None:\n",
    "            cc = 'r'\n",
    "            if cs is not None:\n",
    "                cc = cs.clusters\n",
    "            ax.scatter(filtered_df[c1], filtered_df[c2], filtered_df[c3], c=cc, cmap=\"Set1\")\n",
    "        else:\n",
    "            filtered_df['target'] = MS_TARGET.loc[filtered_df.index]\n",
    "            \n",
    "            f_on = filtered_df[filtered_df['target'] == True]\n",
    "            f_off = filtered_df[filtered_df['target'] == False]\n",
    "            ax.scatter(f_on[c1], f_on[c2], f_on[c3], c=\"red\", marker='X', alpha=.15)\n",
    "            ax.scatter(f_off[c1], f_off[c2], f_off[c3], c=\"blue\", marker='o', alpha=.15)\n",
    "        # if cs.num_clusters() > 0:\n",
    "        #     projected = all_proj\n",
    "        #     ax.scatter(projected[col1_index], projected[col2_index], projected[col3_index], c=cs.clusters, marker='x', cmap='Set1')\n",
    "    elif target_cluster is not None and cs is not None:\n",
    "        pidx = np.where(cs.clusters == target_cluster)[0]\n",
    "        cdf = filtered_df.iloc[pidx]\n",
    "        rgba = _get_color(target_cluster, min_label, max_label)\n",
    "        ax.scatter(cdf[c1], cdf[c2], cdf[c3], c=[rgba])\n",
    "        if cs.num_clusters() > 0:\n",
    "            projected = all_proj[:,pidx]\n",
    "            ax.scatter(projected[col1_index], projected[col2_index], projected[col3_index], c=[rgba], marker='x')\n",
    "\n",
    "    if subspace_bases is not None:\n",
    "        patches = []\n",
    "        for label in cs.cluster_dims:\n",
    "            rgba = _get_color(label, min_label, max_label)\n",
    "            patch = matplotlib.patches.Patch(\n",
    "                color=rgba,\n",
    "                label=str(label) +\n",
    "                      \": dim=\" + str(cs.cluster_dims[label]) +\n",
    "                      \"#fit=\" + str(cluster_counts.get(label, 0)))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # _vec_srcs = [(shared_df, 'r'), (all_df, 'b')]\n",
    "    # _vec_srcs = [(all_df, 'b')]\n",
    "    _vec_srcs = []\n",
    "    for _vec_src, color in _vec_srcs:\n",
    "        if _vec_src is not None:\n",
    "            print(\"PLOT ALL_DF\")\n",
    "            if c1 in _vec_src.columns and c2 in _vec_src.columns and c3 in _vec_src.columns:\n",
    "                # Argh duplicates.\n",
    "                col_list = [c1]\n",
    "                if c2 not in col_list:\n",
    "                    col_list.append(c2)\n",
    "                if c3 not in col_list:\n",
    "                    col_list.append(c3)\n",
    "\n",
    "                print(\"IM PLOTTING AHHH\")\n",
    "                likely_candidates = _vec_src[_vec_src[col_list].sum(axis=1) > 5000][col_list]\n",
    "                print(likely_candidates)\n",
    "                for index, row in likely_candidates.iterrows():\n",
    "                    scale_factor = float('inf') \n",
    "                    if row[c1] != 0:\n",
    "                        scale_factor = min(scale_factor, maxx/row[c1])\n",
    "                    if row[c2] != 0:\n",
    "                        scale_factor = min(scale_factor, maxy/row[c2])\n",
    "                    if row[c3] != 0:\n",
    "                        scale_factor = min(scale_factor, maxz/row[c3]) \n",
    "                    if scale_factor == float('inf'):\n",
    "                        scale_factor = 1\n",
    "                    ax.scatter(row[c1] * scale_factor, \n",
    "                               row[c2] * scale_factor, \n",
    "                               row[c3] * scale_factor, c=color, marker='+', s=500)\n",
    "                    ax.text(row[c1] * scale_factor, row[c2] * scale_factor, row[c3] * scale_factor, index, color='red')\n",
    "\n",
    "    plt.title(title)\n",
    "    xseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c1][\"species\"]\n",
    "    yseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c2][\"species\"]\n",
    "    zseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c3][\"species\"]\n",
    "    \n",
    "    if len(xseries) > 0 and len(yseries) > 0 and len(zseries) > 0:\n",
    "        xlabel = \"\\n\"+xseries.iloc[0] + \"\\n\" + c1\n",
    "        ylabel = \"\\n\"+yseries.iloc[0] + \"\\n\" + c2\n",
    "        zlabel = \"\\n\"+zseries.iloc[0] + \"\\n\" + c3\n",
    "    else:\n",
    "        xlabel = str(c1)\n",
    "        ylabel = str(c2)\n",
    "        zlabel = str(c3)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_zlabel(zlabel)\n",
    "    ax.set_xlim([0, maxx * 1.5 + 100])\n",
    "    ax.set_ylim([0, maxy * 1.5 + 100])\n",
    "    ax.set_zlim([0, maxz * 1.5 + 100])\n",
    "    \n",
    "    if subspace_bases is not None:\n",
    "        plt.legend(handles=patches)\n",
    "        \n",
    "    if subplot_ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bfd19-9042-4178-85eb-5d4ff4181694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf9b59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7750c5be464308af4ba205a2c3625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('all', 'Acetivibrio', 'Acetobacter', 'Acidaminococcus', 'Acinetobacter', 'Actinomyces', 'Akk…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea29786c95db43bb9c07928ab572d4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(layout=Layout(width='50%'), options=(('Bacteroides uniformis(G000154205)', 'G000154205'), ('Bacteroid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c4b3d4acf844548156f50a04f001a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=1, layout=Layout(width='50%'), options=(('Bacteroides uniformis(G000154205)', 'G000154205'), ('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc120805f3b94aa9823938ae430b5caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=2, layout=Layout(width='50%'), options=(('Bacteroides uniformis(G000154205)', 'G000154205'), ('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48346e9cb0347428edde857d98efc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Plot', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb85742ba4245af8ac707318b20b5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Calculate', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9987a3932c004fa29bb59f3081334ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Calculate DetNMF', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fd9166ab6e4cc7a3f46559709acdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='N Components')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435287b131fe472e9e97f73c6c339791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.05, description='Alpha Scale', max=1.0, readout_format='.3f', step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631d3c016fc8428989e716a37965b10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=5000, description='Iterations', max=50000, min=100, step=100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9dd8e677454956a92c573249d5e230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b95c1201cb547148bd9414b3c732385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Fix', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d348f5d5d98545428bf98797f117c045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcc938da6034fcd8d1d8e4454bb0af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Plot Fix', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441e386867c64698a855662168366122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Plot Fix Resids', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea45072afd4afeb1b75cd6e6331d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save Bioms', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genus_widget = widgets.Dropdown(options=list_genera(df, woltka_meta_df, min_sample_count=10, min_genus_count=MIN_GENUS_COUNT))\n",
    "\n",
    "axis1 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "axis2 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "axis3 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "\n",
    "plot = widgets.Button(description='Plot')\n",
    "calculate = widgets.Button(description='Calculate')\n",
    "calculate_detnmf = widgets.Button(description='Calculate DetNMF')\n",
    "calculate_detnmf_components = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='N Components',\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "calculate_detnmf_alpha = widgets.FloatSlider(\n",
    "    value=0.05,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.001,\n",
    "    description='Alpha Scale',\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.3f'\n",
    ")\n",
    "calculate_detnmf_iterations = widgets.IntSlider(\n",
    "    value=5000,\n",
    "    min=100,\n",
    "    max=50000,\n",
    "    step=100,\n",
    "    description=\"Iterations\",\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "save = widgets.Button(description='Save')\n",
    "save_bioms = widgets.Button(description='Save Bioms')\n",
    "fix = widgets.Button(description='Fix')\n",
    "plot_fix = widgets.Button(description=\"Plot Fix\")\n",
    "plot_fix_resids = widgets.Button(description=\"Plot Fix Resids\")\n",
    "output = widgets.Output()\n",
    "\n",
    "state = {}\n",
    "\n",
    "def filter_df(df, genus, min_genus_count=MIN_GENUS_COUNT):\n",
    "    if genus=='all':\n",
    "        refs_df = list_woltka_refs(df, woltka_meta_df)\n",
    "    else:\n",
    "        refs_df = list_woltka_refs(df, woltka_meta_df, genus)\n",
    "    genomes = refs_df['#genome'].tolist()\n",
    "    if len(genomes) == 0 and genus =='all':\n",
    "        print(\"Probably 16S, falling back to raw table\")\n",
    "        filtered_df = df.copy()\n",
    "    else:\n",
    "        filtered_df = df[genomes]\n",
    "        filtered_df_sum = filtered_df.sum(axis=1)\n",
    "        filtered_df = filtered_df[filtered_df_sum >= min_genus_count]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Define a function that updates the content of y based on what we select for x\n",
    "def update_genus(*args):\n",
    "    genus = genus_widget.value\n",
    "    if genus=='all':\n",
    "        refs_df = list_woltka_refs(df, woltka_meta_df)\n",
    "    else:\n",
    "        refs_df = list_woltka_refs(df, woltka_meta_df, genus)\n",
    "    \n",
    "    genomes = refs_df['#genome'].tolist()\n",
    "    species = refs_df['species'].tolist()\n",
    "    choices = []\n",
    "\n",
    "    if genus == 'all' and len(genomes) == 0:\n",
    "        print(\"Probably a 16S table, falling back to sorted columns\")\n",
    "        col_sums = df.sum()\n",
    "        col_sums.name='total'\n",
    "        col_sums = col_sums.sort_values(ascending=False)\n",
    "        genomes=list(col_sums.index)\n",
    "        species=list(genomes)\n",
    "        \n",
    "    for i in range(len(genomes)):\n",
    "        choices.append((species[i] + \"(\" + genomes[i] + \")\", genomes[i]))\n",
    "\n",
    "    axis1.options = choices\n",
    "    axis2.options = choices\n",
    "    axis3.options = choices\n",
    "    axis1.value = genomes[min(0, len(genomes)-1)]\n",
    "    axis2.value = genomes[min(1, len(genomes)-1)]\n",
    "    axis3.value = genomes[min(2, len(genomes)-1)]\n",
    "    \n",
    "    filtered_df = filter_df(df, genus)\n",
    "\n",
    "    state.clear()\n",
    "    state[\"genus\"] = genus\n",
    "    state[\"filtered_df\"] = filtered_df\n",
    "    state[\"cluster_manager\"] = ClusterManager(filtered_df, filtered_df.shape[1])\n",
    "    state[\"breakdown\"] = data_transform.get(genus)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if len(genomes) < 2:\n",
    "            print(genus, \"Not enough reference genomes to do clustering\\n\")\n",
    "            calculate.disabled=True\n",
    "        elif len(filtered_df) <= 10:\n",
    "            print(genus, \"Not enough reads to do clustering\\n\")\n",
    "            calculate.disabled=True\n",
    "        else:\n",
    "            calculate.disabled=False\n",
    "\n",
    "genus_widget.observe(update_genus, names='value')\n",
    "update_genus()\n",
    "\n",
    "display(genus_widget)\n",
    "display(axis1)\n",
    "display(axis2)\n",
    "display(axis3)\n",
    "display(plot)\n",
    "display(calculate)\n",
    "display(calculate_detnmf)\n",
    "display(calculate_detnmf_components)\n",
    "display(calculate_detnmf_alpha)\n",
    "display(calculate_detnmf_iterations)\n",
    "display(save)\n",
    "display(fix)\n",
    "display(output)\n",
    "display(plot_fix)\n",
    "display(plot_fix_resids)\n",
    "display(save_bioms)\n",
    "\n",
    "def plot_click(button):\n",
    "    print('\"' + state[\"genus\"] + '\":[\"' + axis1.value + '\", \"' + axis2.value + '\", \"' + axis3.value + '\"],')\n",
    "    filtered_df = state[\"filtered_df\"]\n",
    "    plot_scatter3(\n",
    "        state[\"filtered_df\"], \n",
    "        state.get(\"cluster_manager\"), \n",
    "        state[\"genus\"],\n",
    "        axis1.value,\n",
    "        axis2.value,\n",
    "        axis3.value\n",
    "    )\n",
    "    \n",
    "def plot_fix_click(button):\n",
    "    fixed_df = state.get(\"fixed_df\")\n",
    "    if fixed_df is None:\n",
    "        return\n",
    "    plot_scatter3(\n",
    "        fixed_df,\n",
    "        None, # state.get(\"cluster_manager\"), \n",
    "        state[\"genus\"],\n",
    "        fixed_df.columns[0],\n",
    "        fixed_df.columns[min(1, fixed_df.shape[1]-1)],\n",
    "        fixed_df.columns[min(2, fixed_df.shape[1]-1)]\n",
    "    )\n",
    "    \n",
    "def _plot_fix_resids(ax, genus, l1_resids, vlines=None):\n",
    "    # sort the data in ascending order\n",
    "    x = np.sort(l1_resids)\n",
    "\n",
    "    # get the cdf values of y\n",
    "    N = len(l1_resids)\n",
    "    y = np.arange(N) / float(N)\n",
    "\n",
    "    # plotting\n",
    "    ax.set_xlabel('x-axis')\n",
    "    ax.set_ylabel('y-axis')\n",
    "    ax.set_title(genus + 'L1 Residuals CDF')\n",
    "    ax.plot(x, y, marker='o')\n",
    "    ax.axhline(y=0.8, color='gray', linestyle=':')\n",
    "    ax.axhline(y=0.9, color='gray', linestyle=':')\n",
    "    if vlines is not None:\n",
    "        for vline in vlines:\n",
    "            ax.axvline(x=vline, color='gray', linestyle=\":\")\n",
    "        \n",
    "def plot_fix_resids_click(button):\n",
    "    if state.get(\"l1_resids\") is None:\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "    _plot_fix_resids(axs[0], state.get(\"genus\"), state.get(\"l1_resids\"))\n",
    "    _plot_fix_resids(axs[1], state.get(\"genus\"), state.get(\"l1_resid_pcts\"), vlines=[0.05, 0.10])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def _calculate(to_cluster, genus):\n",
    "    num_dims = 10 # Beware, this is completely dataset dependent!\n",
    "    if genus != 'all':\n",
    "        num_dims = to_cluster.shape[1]\n",
    "    cm = ClusterManager(to_cluster, num_dims)\n",
    "    rc = RecursiveClusterer()\n",
    "    rc.run(cm)\n",
    "    return {\"cluster_manager\": cm, \"clusterer\": rc}\n",
    "\n",
    "def calculate_click(button):\n",
    "    updated = _calculate(state[\"filtered_df\"], state[\"genus\"])\n",
    "    state.update(updated)\n",
    "    \n",
    "def calculate_detnmf_click(button):\n",
    "    to_cluster = state[\"filtered_df\"]\n",
    "    genus = state[\"genus\"]\n",
    "    \n",
    "    num_components = calculate_detnmf_components.value\n",
    "    alpha_scale = calculate_detnmf_alpha.value\n",
    "    iterations = calculate_detnmf_iterations.value\n",
    "    \n",
    "    cm = ClusterManager.apply_bases_from_detnmf(to_cluster, num_components, alpha_scale=alpha_scale, iterations=iterations)\n",
    "    state.update({\"cluster_manager\": cm, \"clusterer\": None})\n",
    "    \n",
    "def _save(genus, filtered_df, cm, data_path):\n",
    "    if cm == None:\n",
    "        print(\"No clusters to save\")\n",
    "        return\n",
    "\n",
    "    cs = cm.cluster_state\n",
    "    subspace_bases = cm.calc_subspace_bases()\n",
    "\n",
    "    surfaces = []\n",
    "    for key in subspace_bases:\n",
    "        basis = subspace_bases[key]\n",
    "        basis_df = pd.DataFrame(basis, index=filtered_df.columns)\n",
    "        print(basis_df)\n",
    "        surfaces.append(basis_df.to_json())\n",
    "    \n",
    "    data_transform[genus] = surfaces\n",
    "    only_genus = {genus: surfaces}\n",
    "\n",
    "    with open(data_path, 'w') as outfile:\n",
    "        print(\"Saving to: \" + data_path)\n",
    "        json.dump(only_genus, outfile)\n",
    "    \n",
    "def save_click(button):\n",
    "    _save(state[\"genus\"], state[\"filtered_df\"], state[\"cluster_manager\"], DATA_TRANSFORM_PATH)\n",
    "    \n",
    "def save_bioms_click(button):\n",
    "    genus = state[\"genus\"]\n",
    "  \n",
    "    # Filtered tables have a min_genus_count of 500, which confuses external code\n",
    "#     f_table = biom.table.Table(state[\"filtered_df\"].to_numpy().T, state[\"filtered_df\"].columns, state[\"filtered_df\"].index)\n",
    "#     with biom.util.biom_open('results/filtered_'+genus+'.biom', 'w') as f:\n",
    "#         f_table.to_hdf5(f, \"filtered table\")\n",
    "\n",
    "#     f_table = biom.table.Table(state[\"fixed_df\"].to_numpy().T, state[\"fixed_df\"].columns, state[\"fixed_df\"].index)\n",
    "#     with biom.util.biom_open('results/fixed_'+genus+'.biom', 'w') as f:\n",
    "#         f_table.to_hdf5(f, \"fixed table\")\n",
    "        \n",
    "    # redo the filtering without a minimum genus count\n",
    "    full_filtered = filter_df(df, genus, min_genus_count=0)\n",
    "    f_table = biom.table.Table(full_filtered.to_numpy().T, full_filtered.columns, full_filtered.index)\n",
    "    with biom.util.biom_open('results/filtered_'+genus+'.biom', 'w') as f:\n",
    "        f_table.to_hdf5(f, \"filtered table\")\n",
    "        \n",
    "    full_fixed = _fix(state[\"cluster_manager\"], full_filtered)[\"fixed_df\"]\n",
    "    f_table = biom.table.Table(full_fixed.to_numpy().T, full_fixed.columns, full_fixed.index)\n",
    "    with biom.util.biom_open('results/fixed_'+genus+'.biom', 'w') as f:\n",
    "        f_table.to_hdf5(f, \"fixed table\")\n",
    "        \n",
    "\n",
    "def _fix(cm, filtered_df):\n",
    "    if cm is None:\n",
    "        print(\"No clusters to fix!\")\n",
    "        return {}\n",
    "    cs = cm.cluster_state\n",
    "    subspace_bases = cm.calc_subspace_bases()\n",
    "    cluster_counts = cs.get_cluster_counts()\n",
    "    \n",
    "    if len(subspace_bases) == 0:\n",
    "        print(\"Could not identify species vectors\")\n",
    "        return {}\n",
    "    # L1 normalize subspace bases.\n",
    "    print(subspace_bases)\n",
    "    for c_id in subspace_bases:\n",
    "        print(c_id)\n",
    "        for col in range(subspace_bases[c_id].shape[1]):\n",
    "            l1_len = np.sum(np.abs(subspace_bases[c_id][:,col]))\n",
    "            if l1_len != 0:\n",
    "                subspace_bases[c_id][:,col] = subspace_bases[c_id][:,col] / l1_len\n",
    "            if np.sum(subspace_bases[c_id][:,col]) < 0:\n",
    "                subspace_bases[c_id][:,col] = -subspace_bases[c_id][:,col]\n",
    "\n",
    "    # TODO FIXME HACK:  Find a way to include SVs from all dims of higher dim clusters rather than just the first\n",
    "    full_basis = np.stack([subspace_bases[c][:,0] for c in subspace_bases], axis=1)\n",
    "    basis_counts = [cluster_counts[c] for c in subspace_bases]\n",
    "    full_basis = merge_similar_vectors(full_basis, basis_counts, same_thresh_degrees=5)\n",
    "    \n",
    "    out_pts = []\n",
    "    l1_resids = []\n",
    "    l1_resid_pcts = []\n",
    "    max_resid = 0\n",
    "    max_resid_pct = 0\n",
    "    for i in range(filtered_df.shape[0]):\n",
    "        pt = filtered_df.iloc[i].T.to_numpy()\n",
    "        output_pt, l2_resid = scipy.optimize.nnls(full_basis, pt)\n",
    "        nnls_proj = np.matmul(full_basis, output_pt)\n",
    "        l1_resid = np.sum(np.abs(pt - nnls_proj))\n",
    "\n",
    "        output_pt = pd.DataFrame(output_pt).T\n",
    "        output_pt.index = [filtered_df.index[i]]\n",
    "\n",
    "        l1_resids.append(l1_resid)\n",
    "        length = np.sum(np.abs(pt))\n",
    "        l1_resid_pct = l1_resid / length\n",
    "        l1_resid_pcts.append(l1_resid_pct)\n",
    "        \n",
    "            \n",
    "        max_resid = max(max_resid, abs(l1_resid))\n",
    "        if pt.sum() > 0: \n",
    "            max_resid_pct = max(max_resid_pct, abs(l1_resid)/pt.sum())\n",
    "\n",
    "        out_pts.append(output_pt)\n",
    "    \n",
    "    output_df = pd.concat(out_pts)\n",
    "    output_df = output_df.fillna(0)\n",
    "    output_df.columns = list([\"SV\"+str(i) for i in range(len(output_df.columns))])\n",
    "    \n",
    "    return {\"fixed_df\": output_df, \"l1_resids\": l1_resids, \"l1_resid_pcts\": l1_resid_pcts}\n",
    "    \n",
    "def fix_click(button):\n",
    "    updated = _fix(state.get(\"cluster_manager\"), state[\"filtered_df\"])\n",
    "    state.update(updated)\n",
    "    \n",
    "plot.on_click(plot_click)\n",
    "calculate.on_click(calculate_click)\n",
    "calculate_detnmf.on_click(calculate_detnmf_click)\n",
    "save.on_click(save_click)\n",
    "save_bioms.on_click(save_bioms_click)\n",
    "fix.on_click(fix_click)\n",
    "plot_fix.on_click(plot_fix_click)\n",
    "plot_fix_resids.on_click(plot_fix_resids_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41eff84c-b23b-4acc-b8c0-8a0213c053ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_calculate(prefix):\n",
    "    genera = list_genera(df, woltka_meta_df, min_sample_count=10, min_genus_count=MIN_GENUS_COUNT)\n",
    "    \n",
    "    for genus in genera:\n",
    "        if genus == \"all\":\n",
    "            continue\n",
    "        print(\"Genus:\", genus)\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        \n",
    "        ns = _calculate(filtered_df, genus)\n",
    "        cm = ns[\"cluster_manager\"]\n",
    "        \n",
    "        ns = _fix(cm, filtered_df)\n",
    "        if len(ns) == 0:\n",
    "            # no species vectors\n",
    "            print(\"SKIP\")\n",
    "            continue\n",
    "            \n",
    "        fixed_df = ns[\"fixed_df\"]\n",
    "        l1_resids = ns[\"l1_resids\"]\n",
    "        l1_resid_pcts = ns[\"l1_resid_pcts\"]\n",
    "        \n",
    "        # fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "        # _plot_fix_resids(axs[0], genus, l1_resids)\n",
    "        # _plot_fix_resids(axs[1], genus, l1_resid_pcts, vlines=[0.05, 0.10])\n",
    "        # plt.show()\n",
    "        \n",
    "        _save(genus, filtered_df, cm, \"./results/\" + prefix + \"/\" + genus + \".json\")\n",
    "# bulk_calculate(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6d952d-6ae4-45f8-9831-13880bd74998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_compare(prefix):\n",
    "    genera = list_genera(df, woltka_meta_df, min_sample_count=10, min_genus_count=MIN_GENUS_COUNT)\n",
    "    \n",
    "    calculation_failed = 0\n",
    "    calculation_success = 0\n",
    "    calculation_bad_result = 0\n",
    "    increased_dim = 0\n",
    "    columns_evaluated = 0\n",
    "    columns_dropped = 0\n",
    "\n",
    "    for genus in genera:\n",
    "        if genus == \"all\":\n",
    "            continue\n",
    "        print(\"Genus:\", genus)\n",
    "        file = \"./results/\" + prefix + \"/\" + genus + \".json\"\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        \n",
    "        initial_dim = filtered_df.shape[1]\n",
    "        columns_evaluated += initial_dim\n",
    "        \n",
    "        if not os.path.exists(file):\n",
    "            print(\"File does not exist, calculation failed.\")\n",
    "            calculation_failed += 1\n",
    "            continue\n",
    "        \n",
    "        cm = ClusterManager.apply_bases_from_file(file, filtered_df)\n",
    "        cm.finalize()\n",
    "        \n",
    "        ns = _fix(cm, filtered_df)\n",
    "        if len(ns) == 0:\n",
    "            # no species vectors\n",
    "            print(\"No Species Vectors Were Calculated\")\n",
    "            calculation_failed += 1\n",
    "            continue\n",
    "            \n",
    "        fixed_df = ns[\"fixed_df\"]\n",
    "        l1_resids = ns[\"l1_resids\"]\n",
    "        l1_resid_pcts = ns[\"l1_resid_pcts\"]\n",
    "        \n",
    "        x = np.sort(l1_resid_pcts)\n",
    "        x_80 = l1_resid_pcts[int(.8 * len(l1_resid_pcts))]\n",
    "        x_90 = l1_resid_pcts[int(.9 * len(l1_resid_pcts))]\n",
    "        \n",
    "        print(x_80, x_90)\n",
    "        \n",
    "        final_dim = fixed_df.shape[1]\n",
    "                \n",
    "        if x_80 < 0.05 and final_dim <= initial_dim:\n",
    "            print(\"Success\")\n",
    "            calculation_success += 1\n",
    "            print(\"Dropping\", initial_dim - final_dim, \"columns.\")\n",
    "            columns_dropped += initial_dim - final_dim\n",
    "        elif x_80 >= 0.05:\n",
    "            print(\"Bad Result\")\n",
    "            calculation_bad_result += 1\n",
    "        elif final_dim > initial_dim:\n",
    "            increased_dim += 1        \n",
    "        \n",
    "        # fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "        # _plot_fix_resids(axs[0], genus, l1_resids)\n",
    "        # _plot_fix_resids(axs[1], genus, l1_resid_pcts, vlines=[0.05, 0.10])\n",
    "        # plt.show()\n",
    "    \n",
    "    print(\"Success\", calculation_success)    \n",
    "    print(\"Fail\", calculation_failed)\n",
    "    print(\"Bad Result\", calculation_bad_result)\n",
    "    print(\"Bad Result (Increased Dim)\", increased_dim)\n",
    "    print(\"\\nTotal Dimensionality Reduction:\", columns_dropped, str(int((columns_dropped / columns_evaluated) * 100)) + \"%\")\n",
    "    \n",
    "# bulk_compare(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfb7f70-0a9d-4fd8-a571-ae4402428fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43945b2bec646658597d29c7903b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='80%'), placeholder='split 1 2')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4a83da2a2445498dead35fb13e0fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Command', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cluster_run_click(button):\n",
    "    cmd = cluster_command.value\n",
    "    ss = cmd.split()\n",
    "    f_df = state[\"filtered_df\"]\n",
    "    cm = state[\"cluster_manager\"]\n",
    "    cs = cm.cluster_state\n",
    "\n",
    "    if ss[0] == 'split':\n",
    "        cluster_id = int(ss[1])\n",
    "        cluster_pieces = int(ss[2])\n",
    "        cm.get_split_cluster(cluster_id, cluster_pieces).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'merge':\n",
    "        cluster_id_a = int(ss[1])\n",
    "        cluster_id_b = int(ss[2])\n",
    "        final_dim = int(ss[3])\n",
    "        cm.get_merge_clusters(cluster_id_a, cluster_id_b, final_dim).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'delete' or ss[0] == 'del':\n",
    "        cluster_id = int(ss[1])\n",
    "        cm.get_delete_cluster(cluster_id).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'reassign':\n",
    "        cm.get_reassign_nearest(dim_penalty=10, outlier_thresh=0.10).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'prevalence':\n",
    "        print(df.shape)\n",
    "        print(f_df.shape)\n",
    "        print(\"Approximate \" + state[\"genus\"] + \" prevalence\")\n",
    "        print(str((f_df.shape[0] / df.shape[0]) * 100) + \"%\")\n",
    "        threshold_df = counts_to_presence_absence(df[f_df.columns], 500)\n",
    "        pairwise_eval(threshold_df)\n",
    "    elif ss[0] == 'info':\n",
    "        \n",
    "        if len(ss) >= 2:\n",
    "            cluster_id = int(ss[1])\n",
    "            to_run = [cluster_id]\n",
    "        else:\n",
    "            to_run = range(cm.max_cluster_id())\n",
    "        \n",
    "        for cluster_id in to_run:\n",
    "            idx = cs.clusters == cluster_id\n",
    "\n",
    "            # print(\"Samples In Cluster:\")\n",
    "            # print(f_df.iloc[idx].index)\n",
    "\n",
    "            subspace_bases = cm.calc_subspace_bases()\n",
    "            basis = subspace_bases[cluster_id]\n",
    "            basis_df = pd.DataFrame(basis, index=f_df.columns)\n",
    "\n",
    "            abs_basis = basis_df.abs()\n",
    "            subspace = abs_basis.idxmax()\n",
    "\n",
    "            pdf = pd.DataFrame(data=subspace, columns=[\"#genome\"])\n",
    "            pdf = pdf.merge(woltka_meta_df, on=\"#genome\")\n",
    "\n",
    "            print(\"Approximate Name(s)\", cluster_id)\n",
    "            print(pdf[[\"#genome\", \"species\"]])\n",
    "    elif ss[0] == 'diff':\n",
    "        first = int(ss[1])\n",
    "        second = int(ss[2])\n",
    "        \n",
    "        subspace_bases = cm.calc_subspace_bases()\n",
    "        basis_1 = subspace_bases[first]\n",
    "        basis_2 = subspace_bases[second]\n",
    "        \n",
    "        b1 = basis_1[:,0]\n",
    "        b2 = basis_2[:,0]\n",
    "        \n",
    "        if np.sum(b1) < 0:\n",
    "            b1 = -b1\n",
    "        if np.sum(b2) < 0:\n",
    "            b2 = -b2\n",
    "        \n",
    "        b1_l2norm = np.linalg.norm(b1)\n",
    "        b2_l2norm = np.linalg.norm(b2)\n",
    "        \n",
    "        b1 = b1 / b1_l2norm\n",
    "        b2 = b2 / b2_l2norm\n",
    "        \n",
    "        dot = np.dot(b1,b2)\n",
    "        theta = np.arccos(dot)\n",
    "        print(\"Dot Product:\", dot, \"Diff Angle Theta (Degrees):\", theta * 180/math.pi)        \n",
    "        diffs = np.abs(b2 - b1)\n",
    "        max_diff = np.argmax(diffs)\n",
    "        print(f_df.columns[max_diff],diffs[max_diff], b1[max_diff], b2[max_diff])\n",
    "    elif ss[0] == 'taxi':\n",
    "        new_filt_df = f_df.copy()\n",
    "\n",
    "        for cluster_id in cs.cluster_dims:\n",
    "            idx = cs.clusters == cluster_id\n",
    "        \n",
    "        cluster_dim = cs.cluster_dims[cluster_id]\n",
    "        if cluster_dim == 1:\n",
    "            new_filt_df[\"AX\"+str(cluster_id)]\n",
    "    elif ss[0] == \"load\":\n",
    "        f = ss[1]\n",
    "        # ./results/celeste_ecoli_many.json\n",
    "        # TODO FIXME HACK:  Should I apply it to df or filtered df?\n",
    "        cm = ClusterManager.apply_bases_from_file(f, f_df)\n",
    "        cm.finalize()\n",
    "        state[\"cluster_manager\"] = cm\n",
    "    else:\n",
    "        print(\"Unsupported Command\")\n",
    "\n",
    "cluster_command = widgets.Text(layout=widgets.Layout(width='80%'), placeholder='split 1 2')\n",
    "cluster_run = widgets.Button(description='Run Command')\n",
    "\n",
    "display(cluster_command)\n",
    "display(cluster_run)\n",
    "\n",
    "cluster_run.on_click(cluster_run_click)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568a3879-ff0d-4342-a9a1-6b81f5175528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"cluster_manager\"].cluster_state.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2400498b-2641-4b4d-bd37-b478de01b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_plot(df, genlist=None, suptitle=None):\n",
    "    if genlist is None:\n",
    "        genlist = list_genera(df, woltka_meta_df, min_sample_count=10, min_genus_count=MIN_GENUS_COUNT)\n",
    "    print(len(genlist))\n",
    "    i = 0\n",
    "    fig = None\n",
    "    passing = 0\n",
    "    for genus in genlist:\n",
    "        if genus == 'all':\n",
    "            continue\n",
    "\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        sample_pass = round((filtered_df.shape[0] / df.shape[0]) * 100,2)\n",
    "        print(genus + \", \" + str(sample_pass) + \"%\" + \", \" + imsms_qualitative_class[genus][0])\n",
    "\n",
    "        # if sample_pass < 15:\n",
    "        #     passing += 1\n",
    "        #     continue\n",
    "        \n",
    "        if i == 0:\n",
    "            fig = plt.figure()\n",
    "            if suptitle is not None:\n",
    "                fig.suptitle(suptitle)\n",
    "        ax = fig.add_subplot(2, 3, i+1, projection='3d')\n",
    "        i += 1\n",
    "\n",
    "        classification = \"\\n(Co-Exclusive)\"\n",
    "        if imsms_qualitative_class[genus][0] != \"YES\":\n",
    "            classification = \"\\n(Co-Occurring)\"\n",
    "        plot_scatter3(\n",
    "            filtered_df, \n",
    "            ClusterManager(filtered_df, filtered_df.shape[1]), \n",
    "            genus + \": \" + str(sample_pass) + \"%\" + classification,\n",
    "            imsms_plots[genus][0],\n",
    "            imsms_plots[genus][1],\n",
    "            imsms_plots[genus][2],\n",
    "            subplot_ax = ax\n",
    "        )\n",
    "        \n",
    "        if i == 6:\n",
    "            i = 0\n",
    "            plt.show()\n",
    "    print(passing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca81fe26-61de-4124-8259-28a4c65373dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imsms = BiomTable(\"./dataset/biom/imsms-combined-none.biom\").load_dataframe()\n",
    "# df_finrisk = BiomTable(\"./dataset/biom/finrisk-combined-none.biom\").load_dataframe()\n",
    "# df_sol = BiomTable(\"./dataset/biom/sol_public_99006-none.biom\").load_dataframe()\n",
    "\n",
    "#bulk_plot(df)\n",
    "\n",
    "# bulk_plot(df_imsms, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"iMSMS\")\n",
    "# bulk_plot(df_finrisk, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"FINRISK\")\n",
    "# bulk_plot(df_sol, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"SOL\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d682dd8-b8ec-49e8-8dc2-3b8e8af61a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457532.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"filtered_df\"].sum(axis=1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0222429f-23fe-4a65-9736-7f31913a0cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1    3327\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cm = state[\"cluster_manager\"]\n",
    "\n",
    "full_clusters = cm.apply_active_clustering(df, dim_penalty=5, outlier_thresh=.1)\n",
    "print(full_clusters.value_counts())\n",
    "\n",
    "full_df = df[cm.data_cols]\n",
    "full_cm = ClusterManager(full_df, len(cm.data_cols))\n",
    "full_cm.cluster_state = cm.cluster_state.copy()\n",
    "full_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "\n",
    "# plot_scatter3(full_df, full_cm, \"TEST\", full_df.columns[0],full_df.columns[1],full_df.columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d47c32-e568-4331-96f6-542299994d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m c2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(filtered_df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m c3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mlen\u001b[39m(filtered_df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mplot_scatter3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCONIC TEST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_cluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconic_bases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconic_bases\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mplot_scatter3\u001b[0;34m(filtered_df, cluster_manager, title, c1, c2, c3, target_cluster, subplot_ax, conic_bases)\u001b[0m\n\u001b[1;32m     43\u001b[0m pidx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(cs\u001b[38;5;241m.\u001b[39mclusters \u001b[38;5;241m==\u001b[39m target_cluster)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m cdf \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39miloc[pidx]\n\u001b[0;32m---> 45\u001b[0m maxx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m maxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cdf[c2])\n\u001b[1;32m     47\u001b[0m maxz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cdf[c3])\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# For my next trick, I will convert linear subspaces to conic polyhedrons\n",
    "# Since its not entirely clear how to do this when there is noise, we will use the following idea:\n",
    "# Points on a 1D linear subspace results in an obvious basis vector\n",
    "# Points on a 2D linear subspace correspond to a 1D line segment on the simplex.  The endpoints of that line\n",
    "# segment result in two basis vectors\n",
    "# Points on a 3D linear subspace correspond to a 2D polygon on the simplex.  The \"best\" simplification of\n",
    "# that polygon to a triangle results in three basis vectors.  The best simplification is likely the\n",
    "# triangle with the largest area, though we can try other algorithms.  \n",
    "# For a 4D, it should result in a 3D polyhedron on the simplex, the best simplification of that to a \n",
    "# tetrahedron is the conic polyhedron.  \n",
    "# For a 5D, it should result in a 4-simplex on the the 5D simplex.  Grarghh.  \n",
    "\n",
    "# NOTE:  Finding the largest n simplex in an n-1 dimensional convex hull is NP hard and inapproximable.\n",
    "# RIP.  https://stackoverflow.com/questions/50049658/largest-simplex-in-convex-hull-of-points-in-n-dimensions\n",
    "# We will go ahead and use either brute force or some bs heuristic.  \n",
    "cm = state[\"cluster_manager\"]\n",
    "cs = cm.cluster_state\n",
    "filtered_df = state[\"filtered_df\"]\n",
    "subspace_bases = cm.calc_subspace_bases()\n",
    "all_proj = calc_projected(filtered_df, cs.clusters, subspace_bases)\n",
    "\n",
    "conic_bases = {}\n",
    "for cluster_id in subspace_bases:\n",
    "    if subspace_bases[cluster_id].shape[1] == 1:\n",
    "        conic_bases[cluster_id] = subspace_bases[cluster_id]\n",
    "    else:\n",
    "        print(\"Cluster ID:\", cluster_id, \"Dim:\", cs.cluster_dims[cluster_id])\n",
    "        \n",
    "        # Grab points in the cluster\n",
    "        idx = cs.clusters == cluster_id\n",
    "        cluster_proj = all_proj[:,idx]\n",
    "        print(cluster_proj.shape)\n",
    "        \n",
    "        # Push them out to the simplex (L1 normalize them)\n",
    "        for i in range(cluster_proj.shape[1]):\n",
    "            c_pt = cluster_proj[:,i]\n",
    "            if c_pt.sum() != 0:\n",
    "                c_pt = c_pt/c_pt.sum()\n",
    "            cluster_proj[:,i] = c_pt\n",
    "        \n",
    "        M = subspace_bases[cluster_id]\n",
    "        cluster_proj_low_d = np.matmul(M.T, cluster_proj).T        \n",
    "        \n",
    "        # Since we've pushed everything to the simplex, qhull's convex hull will say we are too\n",
    "        # low dimension.  We add a point at the origin to get it to give us a clean set of points.\n",
    "        \n",
    "        cluster_proj_low_d = np.vstack([cluster_proj_low_d, np.zeros(cluster_proj_low_d.shape[1])])\n",
    "        print(cluster_proj_low_d.shape)\n",
    "        hull = scipy.spatial.ConvexHull(cluster_proj_low_d)\n",
    "        \n",
    "        print(hull.vertices)\n",
    "        hull_verts = hull.vertices[hull.vertices != cluster_proj_low_d.shape[0]-1]\n",
    "        print(hull_verts)\n",
    "        plt.plot(cluster_proj_low_d[hull_verts,0], cluster_proj_low_d[hull_verts,1], 'ro')\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(cluster_proj_low_d[simplex, 0], cluster_proj_low_d[simplex, 1], 'k-')\n",
    "        plt.show()\n",
    "        \n",
    "        if subspace_bases[cluster_id].shape[1] == 2:\n",
    "            if len(hull_verts) != 2:\n",
    "                print(\"Dangit hull verts\")\n",
    "                print(hull_verts)\n",
    "                print(all_proj[:,idx][:, hull_verts])\n",
    "                raise Exception(\"Aww I thought it was working, check coplanar points in convex hull?\")\n",
    "            \n",
    "            idx = cs.clusters == cluster_id\n",
    "            cluster_proj = all_proj[:,idx]\n",
    "            \n",
    "            conic_bases[cluster_id] = cluster_proj[:, hull_verts]\n",
    "            print(conic_bases[cluster_id])\n",
    "            print(subspace_bases[cluster_id])\n",
    "            \n",
    "        if subspace_bases[cluster_id].shape[1] == 3:\n",
    "            # Ugh.  Find largest triangle in a convex hull.  At least its more plausible in 3D than 4+\n",
    "            # A possible optimization for this case is here:\n",
    "            # https://stackoverflow.com/questions/1621364/how-to-find-largest-triangle-in-convex-hull-aside-from-brute-force-search/1621913#1621913\n",
    "            # Area of triangle by heron's formula\n",
    "            \n",
    "            largest_area_idx = [-1,-1,-1]\n",
    "            largest_area_sq = 0\n",
    "            for i in hull_verts:\n",
    "                for j in hull_verts:\n",
    "                    for k in hull_verts:\n",
    "                        pt_i = cluster_proj[:,i]\n",
    "                        pt_j = cluster_proj[:,j]\n",
    "                        pt_k = cluster_proj[:,k]\n",
    "                        a = np.linalg.norm(pt_j - pt_i)\n",
    "                        b = np.linalg.norm(pt_k - pt_j)\n",
    "                        c = np.linalg.norm(pt_i - pt_k)\n",
    "                        p = (a+b+c)/2\n",
    "                        area_sq = p * (p-a) * (p-b) * (p-c)\n",
    "                        if area_sq > largest_area_sq:\n",
    "                            largest_area_idx = [i,j,k]\n",
    "                            largest_area_sq = area_sq\n",
    "            \n",
    "            # conic_bases[cluster_id] = cluster_proj[:, largest_area_idx]\n",
    "            conic_bases[cluster_id] = cluster_proj[:, hull_verts]\n",
    "            print(conic_bases[cluster_id])\n",
    "            print(subspace_bases[cluster_id])\n",
    "            \n",
    "c1 = min(3, len(filtered_df.columns)-1)\n",
    "c2 = min(1, len(filtered_df.columns)-1)\n",
    "c3 = min(4, len(filtered_df.columns)-1)\n",
    "plot_scatter3(filtered_df, cm, \"CONIC TEST\", filtered_df.columns[c1],filtered_df.columns[c2],filtered_df.columns[c3], target_cluster=7, conic_bases=conic_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8c33c-5b14-413a-b181-8959eb7741a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to transform to a linear subspace, not a conic polyhedron, ugh...\n",
    "# 𝑌=(𝑀𝑇𝑀)^(−1) * 𝑀𝑇𝑊\n",
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n",
    "# for a way to project onto a conic polyhedron instead\n",
    "# (of course, we still need to compute the conic polyhedron rather than just the subspaces. Ugh.)\n",
    "cm = state[\"cluster_manager\"]\n",
    "cs = cm.cluster_state\n",
    "filtered_df = state[\"filtered_df\"]\n",
    "\n",
    "#Blah, should make this an enum\n",
    "USE_REFERENCE_BASIS = True\n",
    "USE_CALCULATED_BASIS = False\n",
    "\n",
    "if USE_REFERENCE_BASIS:\n",
    "    full_basis = (all_df.copy() / all_df.sum(axis=0)).T.to_numpy()\n",
    "elif USE_CALCULATED_BASIS:\n",
    "    subspace_bases = cm.calc_subspace_bases()\n",
    "\n",
    "    for c_id in subspace_bases:\n",
    "        for col in range(subspace_bases[c_id].shape[1]):\n",
    "            # If we are to maintain read counts in the transformed space, basis vectors must be L1 normalized\n",
    "            # This also negates ones which point in overall negative directions.  \n",
    "            l1_len = np.sum(subspace_bases[c_id][:,col])\n",
    "            if l1_len != 0:\n",
    "                subspace_bases[c_id][:,col] = subspace_bases[c_id][:,col] / l1_len\n",
    "\n",
    "    print(subspace_bases)\n",
    "    full_basis = np.concatenate([subspace_bases[c] for c in subspace_bases], axis=1)\n",
    "\n",
    "print(full_basis)\n",
    "\n",
    "# Find all nearly parallel/anti-parallel vectors, probably have to collapse these or we'll have \n",
    "# extreme numerical instabilities.  \n",
    "SAMENESS_THRESH = 0.9\n",
    "print(\"Basis Shape:\", full_basis.shape)\n",
    "sames = {}\n",
    "for i in range(full_basis.shape[1]):\n",
    "    arr = []\n",
    "    for j in range(full_basis.shape[1]):\n",
    "        # TODO: Ack, basis is L1 normalized, needs to be L2 normalized to make the sameness thresh meaningful\n",
    "        dp = np.dot(full_basis[:,i], full_basis[:,j])\n",
    "        if j > i and dp > SAMENESS_THRESH or dp < -SAMENESS_THRESH:\n",
    "            print(\"Bases: \", i, j, \" are nearly identical\")\n",
    "            # TODO: If there are 3+ axes that are the same, this code won't work right.\n",
    "            sames[j] = i\n",
    "        arr.append(dp)\n",
    "    # print(arr)\n",
    "    \n",
    "# Maybe would be better to do a weighted average of near identical vectors, but for now,\n",
    "# we'll just take the first one, since we are always more confident in vectors towards the beginning of\n",
    "# the list based on the lower dimension and increased number of samples.  \n",
    "for same_key in sames:\n",
    "    full_basis[:, same_key] = full_basis[:, sames[same_key]]\n",
    "\n",
    "vec_index = 0\n",
    "for cluster_id in subspace_bases:\n",
    "    for col in range(subspace_bases[cluster_id].shape[1]):\n",
    "        if vec_index in sames:\n",
    "            subspace_bases[cluster_id][:, col] = full_basis[:, sames[vec_index]]\n",
    "        vec_index += 1\n",
    "\n",
    "# Unclear what to do with outlier points which are either unknown species vectors or rare co-occurrence of\n",
    "# identified species vectors.  We can build a full basis of the identified species vectors to account for\n",
    "# co-occurrence, but our basis vectors for each subspace are not orthogonal to each other\n",
    "# This can result in underconstrained solutions and numerical instability.  If we use Graham-Schmidt \n",
    "# orthonormalization or something similar on our basis vectors in the order we wish to assign them \n",
    "# weight, (and our spaces are sorted by number of dimensions and number of points, so we are assigning\n",
    "# most common first), that may break ties in a clear way.  Majority of numerical instability probably\n",
    "# comes from having multiple estimates of the same species vector, (A 1d vector and a 2d surface overlapping\n",
    "# would do that, as would two 2d surfaces that intersect)\n",
    "solver_cols = {}\n",
    "Ms = {}\n",
    "\n",
    "M = full_basis\n",
    "keep_cols = [i for i in range(full_basis.shape[1]) if i not in sames]\n",
    "M = M[:, keep_cols]\n",
    "cols = [\"ax\" + str(c) for c in keep_cols]\n",
    "\n",
    "solver_cols[OUTLIER_CLUSTER_ID] = cols\n",
    "Ms[OUTLIER_CLUSTER_ID] = M\n",
    "tot_vecs = 0\n",
    "for cluster_id in subspace_bases:\n",
    "    M_i = subspace_bases[cluster_id]\n",
    "    Ms[cluster_id] = M_i\n",
    "    solver_cols[cluster_id] = []\n",
    "    for i in range(subspace_bases[cluster_id].shape[1]):\n",
    "        if tot_vecs in sames:\n",
    "            solver_cols[cluster_id].append(\"ax\" + str(sames[tot_vecs]))\n",
    "        else:\n",
    "            solver_cols[cluster_id].append(\"ax\" + str(tot_vecs))\n",
    "        tot_vecs += 1\n",
    "\n",
    "# For points which do cluster well, we have orthonormal bases (though we want to switch to non orthogonal\n",
    "# bases of conic polyhedra)\n",
    "to_transform = df\n",
    "full_clusters = cm.apply_active_clustering(to_transform, dim_penalty=5, outlier_thresh=.1)\n",
    "print(full_clusters.value_counts())\n",
    "\n",
    "full_df = to_transform[cm.data_cols]\n",
    "full_cm = ClusterManager(full_df, len(cm.data_cols))\n",
    "full_cm.cluster_state = cm.cluster_state.copy()\n",
    "full_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "\n",
    "out_pts = []\n",
    "max_resid = 0\n",
    "for i in range(to_transform.shape[0]):\n",
    "    pt = to_transform[cm.data_cols].iloc[i].T.to_numpy()\n",
    "    cluster_assignment = full_clusters.iloc[i]\n",
    "    \n",
    "    output_pt_lin_subspace = np.matmul(solvers[cluster_assignment], pt)\n",
    "    output_pt_nnls, l2_resid = scipy.optimize.nnls(Ms[cluster_assignment], pt)\n",
    "    nnls_proj = np.matmul(Ms[cluster_assignment], output_pt_nnls)\n",
    "    l1_resid = np.sum(pt - nnls_proj)\n",
    "    # print(output_pt_lin_subspace, \" vs \", output_pt_nnls, \"(\", resid, \")\")\n",
    "    output_pt = output_pt_nnls                                                   \n",
    "                                                   \n",
    "    output_pt = pd.DataFrame(output_pt).T\n",
    "    output_pt.index = [to_transform.index[i]]\n",
    "    output_pt.columns = solver_cols[cluster_assignment]\n",
    "    # print(pt.sum(), output_pt.sum(axis=1), \"+\", l1_resid)\n",
    "    max_resid = max(max_resid, abs(l1_resid))\n",
    "    if pt.sum() > 0: \n",
    "        max_resid_pct = max(max_resid_pct, abs(l1_resid)/pt.sum())\n",
    "    \n",
    "    out_pts.append(output_pt)\n",
    "\n",
    "print(\"MAX RESIDUAL: \", max_resid)\n",
    "    \n",
    "output_df = pd.concat(out_pts)\n",
    "output_df = output_df.fillna(0)\n",
    "print(output_df)\n",
    "print(output_df.loc[\"S.71801.0073.4.7.17\"])    \n",
    "output_cm = ClusterManager(output_df, output_df.shape[1])\n",
    "output_cm.cluster_state = cm.cluster_state.copy()\n",
    "output_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "output_cm.cluster_state.cluster_dims[-1] = output_df.shape[1]\n",
    "\n",
    "c1 = min(0, len(output_df.columns)-1)\n",
    "c2 = min(1, len(output_df.columns)-1)\n",
    "c3 = min(2, len(output_df.columns)-1)\n",
    "plot_scatter3(output_df, full_cm, \"TEST\", output_df.columns[c1],output_df.columns[c2],output_df.columns[c3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8a56-65ac-4b82-b352-408b45646641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_df.head())\n",
    "c1 = min(0, len(output_df.columns)-1)\n",
    "c2 = min(3, len(output_df.columns)-1)\n",
    "c3 = min(4, len(output_df.columns)-1)\n",
    "plot_scatter3(output_df, full_cm, \"TEST\", output_df.columns[c1],output_df.columns[c2],output_df.columns[c3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae035ee-0eaf-4341-b916-4f446c0c9fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fead6-40cb-41aa-abec-97c050ceb608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a57b8-d12e-44fb-b384-a4e022fdbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.array([[1,1],[5,5],[3,0],[2,1]]) \n",
    "\n",
    "hull = scipy.spatial.ConvexHull(pts)\n",
    "\n",
    "plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "    \n",
    "# plt.plot(pts[hull.vertices,0], pts[hull.vertices,1], 'r--', lw=2)\n",
    "# plt.plot(pts[hull.vertices[0],0], pts[hull.vertices[0],1], 'ro')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ec395-0f49-4e84-be96-73bfdd207d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = state[\"filtered_df\"]\n",
    "plot_scatter3(\n",
    "    state[\"filtered_df\"], \n",
    "    state.get(\"cluster_manager\"), \n",
    "    state[\"genus\"],\n",
    "    axis1.value,\n",
    "    axis2.value,\n",
    "    axis3.value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45ab5e-ecf7-4924-9308-f4a4d1446e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/celeste_ecoli_many.json\") as infile:\n",
    "    data_transform = json.load(infile)\n",
    "    all_bases = []\n",
    "    counter = 0\n",
    "    for subspace in range(len(data_transform[\"all\"])):\n",
    "        conic_basis = pd.read_json(data_transform[\"all\"][subspace])\n",
    "        conic_basis.columns = range(counter, counter + conic_basis.shape[1])\n",
    "        counter += conic_basis.shape[1]\n",
    "        all_bases.append(conic_basis)\n",
    "    conic_basis = pd.concat(all_bases, axis=1)\n",
    "\n",
    "\n",
    "# foo_basis = pd.DataFrame([[100/600, 200/700],[100/600, 0],[400/600, 500/700]])\n",
    "# foo_df = pd.DataFrame([[100, 400],[100, 0],[400, 1000]])\n",
    "\n",
    "# print(transformer.transform(foo_df, foo_basis))\n",
    "\n",
    "\n",
    "# conic_basis = transformer.L1_normalize(conic_basis)\n",
    "# print(conic_basis.loc[[\"G000183345\", \"G000026345\",\"G000026325\", \"G000299455\", \"G000008865\", \"G001283625\", \"G000759795\", \"G001941055\", \"G000009065\"],:])\n",
    "transformed = transformer.transform(df, conic_basis)\n",
    "colors = []\n",
    "for i in transformed.index:\n",
    "    ss = i.split(\".\")\n",
    "    if ss[1].startswith(\"4\"):\n",
    "        colors.append(\"m\")\n",
    "    else:\n",
    "        colors.append(\"b\")\n",
    "\n",
    "plt.scatter(transformed[0], transformed[1], c=colors)\n",
    "plt.show()\n",
    "# plt.scatter(df[\"G000183345\"], df[\"G000026345\"])\n",
    "# plt.show()\n",
    "# plt.scatter(transformed[0], transformed[1])\n",
    "# plt.show()\n",
    "\n",
    "transformed.hist(\"L1_resid\")\n",
    "plt.show()\n",
    "transformed[transformed[\"L1_resid\"] > 50000].hist(\"L1_resid\")\n",
    "plt.show()\n",
    "\n",
    "transformed = transformed.sort_values(\"L1_resid\", ascending=False)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(transformed.head(n=100)[[\"L1_resid\", \"WorstAxis\"]])\n",
    "    \n",
    "foo_df = df.loc[transformed.head(n=200).index]\n",
    "\n",
    "# plot_scatter3(\n",
    "#     foo_df, \n",
    "#     ClusterManager(foo_df, 1),\n",
    "#     state[\"genus\"],\n",
    "#     axis1.value,\n",
    "#     axis2.value,\n",
    "#     axis3.value\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dbd47-1f47-4095-a2d5-6fbd00295c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f79b2-369d-4fbf-b270-e5c4fcec1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra junk for plotting info about the bacteroides samples\n",
    "# bacteroides_sample_info = CSVTable(\"./dataset/biom/14360_20211220-082840.txt\", delimiter=\"\\t\")\n",
    "# bacteroides_sample_info = bacteroides_sample_info.load_dataframe()\n",
    "# bacteroides_sample_info.index = bacteroides_sample_info[\"sample_name\"]\n",
    "\n",
    "# bacteroides_sample_contamination = CSVTable(\"./dataset/biom/bfragilis_divisions.txt\", delimiter=\"\\t\")\n",
    "# bacteroides_sample_contamination = bacteroides_sample_contamination.load_dataframe()\n",
    "\n",
    "# sample_division = bacteroides_sample_contamination[[\"Division\", \"Sample\"]]\n",
    "# tube_ids = sample_division[\"Sample\"].map(lambda x: x.split(\"_\")[0])\n",
    "# sample_division.index = tube_ids\n",
    "# sample_division.index.name = \"tube_id\"\n",
    "\n",
    "# real_filtered = state[\"filtered_df\"]\n",
    "# real_cm = state[\"cluster_manager\"]\n",
    "# try:\n",
    "#     fake_clusters = []\n",
    "#     requested_samples = []\n",
    "#     for sample_id in state[\"filtered_df\"].index:\n",
    "#         tube_id = bacteroides_sample_info.loc[sample_id, \"tube_id\"]\n",
    "#         if str(tube_id) in sample_division.index:\n",
    "#             division = int(sample_division.loc[str(tube_id), \"Division\"]) - 1\n",
    "#             requested_samples.append(sample_id)\n",
    "#         else:\n",
    "#             division = -1\n",
    "#         fake_clusters.append(division)\n",
    "\n",
    "#     print(fake_clusters)\n",
    "#     # state[\"filtered_df\"] = state[\"filtered_df\"].loc[requested_samples]    \n",
    "#     fake_cm = ClusterManager(state[\"filtered_df\"], 1)\n",
    "        \n",
    "#     fake_cm.cluster_state = ClusterState(fake_cm, np.array(fake_clusters), {-1:1, 0:1, 1:1})\n",
    "#     state[\"cluster_manager\"] = fake_cm\n",
    "#     plot_click(None)\n",
    "# finally:\n",
    "#     pass\n",
    "#     # state[\"filtered_df\"] = real_filtered\n",
    "#     # state[\"cluster_manager\"] = real_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197dcc3-189f-45a0-8556-eaadc08bb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import NMF\n",
    "from detnmf.detnmf import run_detnmf, run_caliper_nmf\n",
    "\n",
    "to_process = df\n",
    "# to_process = state[\"filtered_df\"] \n",
    "\n",
    "n_components = 8 #to_process.shape[1]\n",
    "\n",
    "X = to_process.to_numpy() #np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "\n",
    "# model = NMF(n_components=n_components, max_iter=200)\n",
    "# W = model.fit_transform(X)\n",
    "# H = model.components_\n",
    "\n",
    "# print(\"Coefficients\")\n",
    "# print(W)\n",
    "# print(\"Components\")\n",
    "# print(H)\n",
    "\n",
    "# print(np.argmax(H))\n",
    "# print(df.columns[np.argmax(H)])\n",
    "\n",
    "# X = df.to_numpy()\n",
    "# X = np.array([[0, 0], [100, 100], [200, 200], [300, 300], [400, 400], [300, 100], [600, 200], [900, 300], [1200, 400], [1500, 500]])\n",
    "# model = MVCNMF(n_components=2)\n",
    "# A, S = model.fit(X, learning_rate=10)\n",
    "\n",
    "W, H = run_detnmf(X, n_components, alpha_scale=0.01, iterations=15000)\n",
    "# W, H = run_caliper_nmf(X, n_components, W=W, H=H, alpha=0.95, iterations=50000)\n",
    "\n",
    "print(\"Data\")\n",
    "print(X.shape)\n",
    "print(X.sum())\n",
    "print(X)\n",
    "print(\"Coefficients\")\n",
    "print(W)\n",
    "print(W.shape)\n",
    "print(\"Components\")\n",
    "print(H)\n",
    "print(H.shape)\n",
    "print(H.sum(axis=1))\n",
    "\n",
    "for component in range(n_components):\n",
    "    big_val = np.argmax(H[component, :])\n",
    "    print(df.columns[big_val])\n",
    "\n",
    "qq = H.copy()\n",
    "for r in range(qq.shape[0]):\n",
    "    qq[r,:] = qq[r,:] / np.linalg.norm(qq[r,:])\n",
    "\n",
    "basis = H.T\n",
    "basis_df = pd.DataFrame(basis, index=to_process.columns)\n",
    "to_save = {\"all\":[basis_df[[col]].to_json() for col in basis_df.columns]}\n",
    "\n",
    "with open(\"./results/detnmf.json\", 'w') as outfile:\n",
    "    json.dump(to_save, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300bb12-a473-42fd-8674-536f958391e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"filtered_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0aa413-8224-43b1-b296-4248ee12306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = state[\"cluster_manager\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeab299-9a05-426a-94b0-1fbea36fd6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b08146-8b3f-48da-9b24-ac8e9394ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 1):  # more options can be specified also\n",
    "    print(df[df.index.str.contains('zymo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac374f8-1c02-45a2-b74c-5ce2024050be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502440f7-dc4f-4b12-b60e-35c942d70795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c170ad3-b076-41ed-be73-4c00337f923e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed14d89-9f77-48ba-af79-637351fb3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy)\n",
    "print(scipy.optimize.nnls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3110a81-858c-4f48-9881-a184cc0e326b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
