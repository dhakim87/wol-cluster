{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7cfa4-6898-4f51-8633-0fdd86f35977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from table_info import BiomTable, CSVTable\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import test_linear_clustering\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from plot_definitions import is_ms, imsms_plots, imsms_qualitative_class\n",
    "from coexclusion import counts_to_presence_absence, counts_to_dynamic_presence_absence, presence_absence_to_contingency, pairwise_eval\n",
    "import os\n",
    "\n",
    "from test_linear_clustering import recursive_subspacer, iterative_clustering, calc_projected, _fit_a_linear_subspace, _draw_subspace_line, _get_color\n",
    "from linear_subspace_clustering import linear_subspace_clustering, calc_subspace_bases\n",
    "from cluster_manager import ClusterManager, ClusterState, RecursiveClusterer, OUTLIER_CLUSTER_ID\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import json\n",
    "import scipy\n",
    "import data_transform as transformer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "WOLTKA_METADATA_PATH=\"./woltka_metadata.tsv\"\n",
    "DATA_TRANSFORM_PATH=\"./results/jupyter_interactive_foo.json\"\n",
    "\n",
    "ALL_SPECIES_VECS = \"./dataset/biom/species_vectors.biom\"\n",
    "SHARED_SPECIES_VECS = \"./dataset/biom/species_vectors_shared.biom\"\n",
    "AKKER_TABLE=\"./dataset/biom/akkermansia_foobar.biom\"\n",
    "\n",
    "# BIOM_TABLE=\"./dataset/biom/imsms-combined-none.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/finrisk-combined-none.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/sol_public_99006-none.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/bacteroides_isolates.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/staph_aureus.biom\"\n",
    "# BIOM_TABLE=\"./dataset/biom/caitriona_matrix_tubes_none.biom\"\n",
    "BIOM_TABLE=[\n",
    "    \"./dataset/biom/Celeste_Prep_1_1428_samples.biom\",\n",
    "    \"./dataset/biom/Celeste_Prep_2_672_samples.biom\",\n",
    "    \"./dataset/biom/Celeste_Prep_3_936_samples.biom\",\n",
    "    \"./dataset/biom/Celeste_Prep_4_792_samples.biom\"\n",
    "]\n",
    "# BIOM_TABLE=\"./dataset/biom/sahar_asd.biom\"\n",
    "\n",
    "\n",
    "MIN_GENUS_COUNT = 500\n",
    "CONSTANT_SUM_SCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "woltka_meta_table = CSVTable(WOLTKA_METADATA_PATH, delimiter=\"\\t\")\n",
    "woltka_meta_df = woltka_meta_table.load_dataframe()\n",
    "\n",
    "if not isinstance(BIOM_TABLE, list):\n",
    "    BIOM_TABLE = [BIOM_TABLE]\n",
    "\n",
    "all_dfs = []\n",
    "for biom_table in BIOM_TABLE:\n",
    "    bt = BiomTable(biom_table)\n",
    "    df = bt.load_dataframe()\n",
    "    all_dfs.append(df)\n",
    "\n",
    "df = pd.concat(all_dfs).fillna(0)\n",
    "\n",
    "print(df.sum())\n",
    "# all_df = None\n",
    "all_species_vecs = BiomTable(ALL_SPECIES_VECS)\n",
    "all_df = all_species_vecs.load_dataframe()\n",
    "\n",
    "all_df.loc[\"intestini\",:] = (all_df.loc[\"G000431295\",:] + all_df.loc[\"G000230275\",:]) / 2\n",
    "all_df.loc[\"fermentans\",:] = (all_df.loc[\"G000025305\",:] + all_df.loc[\"G900107075\",:] + all_df.loc[\"G900115425\",:]) / 3\n",
    "all_df.loc[\"CAG:196\",:] = (all_df.loc[\"G000433235\",:] + all_df.loc[\"G001917235\",:]) / 2\n",
    "\n",
    "shared_species_vecs = BiomTable(SHARED_SPECIES_VECS)\n",
    "shared_df = shared_species_vecs.load_dataframe()\n",
    "\n",
    "MS_TARGET = None\n",
    "# if \"imsms\" in BIOM_TABLE:\n",
    "#     df['target'] = df.index.map(is_ms)\n",
    "#     MS_TARGET = df['target']\n",
    "#     df = df.drop(['target'], axis=1)\n",
    "try:\n",
    "    with open(DATA_TRANSFORM_PATH) as infile:\n",
    "        data_transform = json.load(infile)\n",
    "except:\n",
    "    print(\"Couldn't open data transform, starting new data transform\")\n",
    "    data_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5d787-fd3b-47ee-979f-13fb2031ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.sum(axis=1)\n",
    "cst = df.divide(df_sum, axis='rows') * 10000\n",
    "\n",
    "if CONSTANT_SUM_SCALE:\n",
    "    df = cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_genera():\n",
    "    woltka_included = woltka_meta_df[woltka_meta_df['#genome'].isin(df.columns)]\n",
    "    vcs = woltka_included['genus'].value_counts()\n",
    "    genera = sorted(vcs[vcs>1].index.astype(str).tolist())\n",
    "    \n",
    "    filt_genera = ['all']\n",
    "    for g in genera:\n",
    "        filtered_df = df[list_woltka_refs(g)['#genome']]\n",
    "        filtered_df_sum = filtered_df.sum(axis=1)\n",
    "        filtered_df = filtered_df[filtered_df_sum >= MIN_GENUS_COUNT]\n",
    "        if len(filtered_df) >= 10:\n",
    "            filt_genera.append(g)\n",
    "    return filt_genera\n",
    "\n",
    "def list_woltka_refs(genus=None):\n",
    "    woltka_included = woltka_meta_df[woltka_meta_df['#genome'].isin(df.columns)]\n",
    "    if genus == None:\n",
    "        refs = woltka_included\n",
    "    else:\n",
    "        refs = woltka_included[woltka_included['genus']==genus]\n",
    "    \n",
    "    filtered_df = df[refs['#genome']]\n",
    "    col_sums = filtered_df.sum()\n",
    "    col_sums.name='total'\n",
    "    refs = refs.join(col_sums, on='#genome')\n",
    "    \n",
    "    refs = refs.reset_index()\n",
    "    refs = refs.sort_values([\"total\", \"#genome\"], ascending=False)[['total', '#genome','species']]\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17190675-fdc6-419f-94aa-2529797859db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter3(filtered_df, cluster_manager, title, c1, c2, c3, target_cluster=None, subplot_ax=None, conic_bases=None):    \n",
    "    filtered_df = filtered_df.copy()\n",
    "    cs = cluster_manager.cluster_state\n",
    "    cluster_counts = cs.get_cluster_counts()\n",
    "    \n",
    "    # print(\"Num Clusters:\", cs.num_clusters())\n",
    "    if cs.num_clusters() > 0:\n",
    "        subspace_bases = calc_subspace_bases(filtered_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "        subspace_bases = {x: pd.DataFrame(subspace_bases[x], index=filtered_df.columns) for x in subspace_bases}\n",
    "        all_proj = calc_projected(filtered_df, cs.clusters, subspace_bases)\n",
    "    elif state['genus'] in data_transform:\n",
    "        subspace_bases = data_transform[state['genus']]\n",
    "        subspace_bases = {i: pd.read_json(subspace_bases[i]) for i in range(len(subspace_bases))}\n",
    "        if len(subspace_bases) == 0:\n",
    "            subspace_bases = None\n",
    "        # print(\"Loaded bases:\", subspace_bases)\n",
    "    else:\n",
    "        subspace_bases = None\n",
    "    \n",
    "    if conic_bases is not None:\n",
    "        conic_bases = {x: pd.DataFrame(conic_bases[x], index=filtered_df.columns) for x in conic_bases}\n",
    "    \n",
    "    if subplot_ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "    else:\n",
    "        ax = subplot_ax\n",
    "\n",
    "    maxx = max(filtered_df[c1])\n",
    "    maxy = max(filtered_df[c2])\n",
    "    maxz = max(filtered_df[c3])\n",
    "    \n",
    "    col1_index = filtered_df.columns.get_loc(c1)\n",
    "    col2_index = filtered_df.columns.get_loc(c2)\n",
    "    col3_index = filtered_df.columns.get_loc(c3)\n",
    "\n",
    "    if target_cluster is not None:\n",
    "        pidx = np.where(cs.clusters == target_cluster)[0]\n",
    "        cdf = filtered_df.iloc[pidx]\n",
    "        maxx = max(cdf[c1])\n",
    "        maxy = max(cdf[c2])\n",
    "        maxz = max(cdf[c3])\n",
    "\n",
    "    if subspace_bases is not None:\n",
    "        min_label = -1\n",
    "        if -1 not in cluster_counts or cluster_counts[-1] == 0:\n",
    "            min_label = 0\n",
    "        max_label = max(subspace_bases)\n",
    "\n",
    "        x = np.linspace(0, maxx, 10)\n",
    "        y = np.linspace(0, maxy, 10)\n",
    "\n",
    "        if conic_bases is not None:\n",
    "            to_draw = conic_bases\n",
    "            if target_cluster is not None:\n",
    "                to_draw = [target_cluster]\n",
    "                \n",
    "            for label in to_draw:\n",
    "                dim = conic_bases[label].shape[1]\n",
    "                for u_idx in range(dim):\n",
    "                    u = conic_bases[label].iloc[:,u_idx].loc[[c1,c2,c3]]\n",
    "                    _draw_subspace_line(ax, u, maxx, maxy, maxz, label, min_label, max_label)\n",
    "        for label in subspace_bases:\n",
    "            dim = subspace_bases[label].shape[1]\n",
    "            if dim == 1 or dim > 2:\n",
    "                # unclear how to draw 3+ d surfaces, so just draw primary axis\n",
    "                u = subspace_bases[label].iloc[:,0].loc[[c1,c2,c3]]\n",
    "                _draw_subspace_line(ax,u,maxx,maxy,maxz,label,min_label,max_label)\n",
    "            if dim == 2:\n",
    "                # the basis vectors are orthogonal\n",
    "                # But when you project them down to this space, their projections may not be orthogonal\n",
    "                # They could even be parallel, or even 0 (Take u,v = 0,0,1,-1 and 0,0,1,1 projected to first three dimensions)\n",
    "                # If they are parallel, we should be drawing a line, not a plane\n",
    "                # If they are 0, we should just draw a single point (happens to be at the origin\n",
    "                u = subspace_bases[label].iloc[:,0].loc[[c1,c2,c3]]\n",
    "                v = subspace_bases[label].iloc[:,1].loc[[c1,c2,c3]]\n",
    "                _draw_subspace_line(ax,u,maxx,maxy,maxz,label,min_label,max_label)\n",
    "                _draw_subspace_line(ax,v,maxx,maxy,maxz,label,min_label,max_label)\n",
    "\n",
    "                npu = np.array(u)\n",
    "                npv = np.array(v)\n",
    "                lenu = np.linalg.norm(u)\n",
    "                lenv = np.linalg.norm(v)\n",
    "                if lenu < 0.01 and lenv < 0.01:\n",
    "                    # Both of the vectors are basically 0.  Can't draw anything.\n",
    "                    continue\n",
    "                elif lenu < 0.01:\n",
    "                    _draw_subspace_line(ax, v, maxx,maxy,maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "                elif lenv < 0.01:\n",
    "                    _draw_subspace_line(ax, u, maxx,maxy,maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "\n",
    "                npu = npu / lenu\n",
    "                npv = npv / lenv\n",
    "\n",
    "                dotproduct = np.dot(npu,npv)\n",
    "                if dotproduct > 0.9 or dotproduct < -0.9:\n",
    "                    print(dotproduct)\n",
    "                    # Vectors are basically parallel.  Can't draw a plane, maybe can draw a line.\n",
    "                    if dotproduct < 0:\n",
    "                        npu = -npu\n",
    "                    npu = (npu + npv) / 2\n",
    "                    _draw_subspace_line(ax, npu, maxx, maxy, maxz, label, min_label, max_label)\n",
    "                    continue\n",
    "\n",
    "                normal = np.cross(u.T, v.T)\n",
    "                # Normal vector gives x*n0 + y*n1 + z*n2 = 0\n",
    "                # Divide everything by n2\n",
    "                # x * n0/n2 + y * n1/n2 + z = 0\n",
    "                # z = -n0/n2 * x - n1 / n2 * y\n",
    "                X, Y = np.meshgrid(x,y)\n",
    "                Z = -normal[0]/normal[2] * X - normal[1] / normal[2] * Y\n",
    "                rgba = _get_color(label, min_label, max_label)\n",
    "                rgba = rgba[0],rgba[1],rgba[2],0.25\n",
    "                surf = ax.plot_surface(X, Y, Z, color=rgba)\n",
    "\n",
    "\n",
    "    if target_cluster is None:\n",
    "        if MS_TARGET is None:\n",
    "            ax.scatter(filtered_df[c1], filtered_df[c2], filtered_df[c3], c=cs.clusters, cmap=\"Set1\")\n",
    "        else:\n",
    "            filtered_df['target'] = MS_TARGET.loc[filtered_df.index]\n",
    "            filtered_df['color'] = cs.clusters\n",
    "            \n",
    "            f_on = filtered_df[filtered_df['target'] == True]\n",
    "            f_off = filtered_df[filtered_df['target'] == False]\n",
    "            ax.scatter(f_on[c1], f_on[c2], f_on[c3], c=f_on['color'], marker='X', cmap=\"Set1\")\n",
    "            ax.scatter(f_off[c1], f_off[c2], f_off[c3], c=f_off['color'], marker='o', cmap=\"Set1\")\n",
    "        # if cs.num_clusters() > 0:\n",
    "        #     projected = all_proj\n",
    "        #     ax.scatter(projected[col1_index], projected[col2_index], projected[col3_index], c=cs.clusters, marker='x', cmap='Set1')\n",
    "    elif target_cluster is not None:\n",
    "        pidx = np.where(cs.clusters == target_cluster)[0]\n",
    "        cdf = filtered_df.iloc[pidx]\n",
    "        rgba = _get_color(target_cluster, min_label, max_label)\n",
    "        ax.scatter(cdf[c1], cdf[c2], cdf[c3], c=[rgba])\n",
    "        if cs.num_clusters() > 0:\n",
    "            projected = all_proj[:,pidx]\n",
    "            ax.scatter(projected[col1_index], projected[col2_index], projected[col3_index], c=[rgba], marker='x')\n",
    "\n",
    "    if subspace_bases is not None:\n",
    "        patches = []\n",
    "        for label in cs.cluster_dims:\n",
    "            rgba = _get_color(label, min_label, max_label)\n",
    "            patch = matplotlib.patches.Patch(\n",
    "                color=rgba,\n",
    "                label=str(label) +\n",
    "                      \": dim=\" + str(cs.cluster_dims[label]) +\n",
    "                      \"#fit=\" + str(cluster_counts.get(label, 0)))\n",
    "            patches.append(patch)\n",
    "    \n",
    "    # _vec_srcs = [(shared_df, 'r'), (all_df, 'b')]\n",
    "    # _vec_srcs = [(all_df, 'b')]\n",
    "    _vec_srcs = []\n",
    "    for _vec_src, color in _vec_srcs:\n",
    "        if _vec_src is not None:\n",
    "            print(\"PLOT ALL_DF\")\n",
    "            if c1 in _vec_src.columns and c2 in _vec_src.columns and c3 in _vec_src.columns:\n",
    "                # Argh duplicates.\n",
    "                col_list = [c1]\n",
    "                if c2 not in col_list:\n",
    "                    col_list.append(c2)\n",
    "                if c3 not in col_list:\n",
    "                    col_list.append(c3)\n",
    "\n",
    "                print(\"IM PLOTTING AHHH\")\n",
    "                likely_candidates = _vec_src[_vec_src[col_list].sum(axis=1) > 5000][col_list]\n",
    "                print(likely_candidates)\n",
    "                for index, row in likely_candidates.iterrows():\n",
    "                    scale_factor = float('inf') \n",
    "                    if row[c1] != 0:\n",
    "                        scale_factor = min(scale_factor, maxx/row[c1])\n",
    "                    if row[c2] != 0:\n",
    "                        scale_factor = min(scale_factor, maxy/row[c2])\n",
    "                    if row[c3] != 0:\n",
    "                        scale_factor = min(scale_factor, maxz/row[c3]) \n",
    "                    if scale_factor == float('inf'):\n",
    "                        scale_factor = 1\n",
    "                    ax.scatter(row[c1] * scale_factor, \n",
    "                               row[c2] * scale_factor, \n",
    "                               row[c3] * scale_factor, c=color, marker='+', s=500)\n",
    "                    ax.text(row[c1] * scale_factor, row[c2] * scale_factor, row[c3] * scale_factor, index, color='red')\n",
    "\n",
    "    plt.title(title)\n",
    "    xseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c1][\"species\"]\n",
    "    yseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c2][\"species\"]\n",
    "    zseries = woltka_meta_df[woltka_meta_df[\"#genome\"] == c3][\"species\"]\n",
    "    \n",
    "    if len(xseries) > 0 and len(yseries) > 0 and len(zseries) > 0:\n",
    "        xlabel = \"\\n\"+xseries.iloc[0] + \"\\n\" + c1\n",
    "        ylabel = \"\\n\"+yseries.iloc[0] + \"\\n\" + c2\n",
    "        zlabel = \"\\n\"+zseries.iloc[0] + \"\\n\" + c3\n",
    "    else:\n",
    "        xlabel = str(c1)\n",
    "        ylabel = str(c2)\n",
    "        zlabel = str(c3)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_zlabel(zlabel)\n",
    "    ax.set_xlim([0, maxx * 1.5 + 100])\n",
    "    ax.set_ylim([0, maxy * 1.5 + 100])\n",
    "    ax.set_zlim([0, maxz * 1.5 + 100])\n",
    "    \n",
    "    if subspace_bases is not None:\n",
    "        plt.legend(handles=patches)\n",
    "        \n",
    "    if subplot_ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bfd19-9042-4178-85eb-5d4ff4181694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the ability to merge similar vectors that come back from clustering.\n",
    "def merge_similar_vectors(vectors, weights, same_thresh_degrees = 5):\n",
    "    def kruskal_top(groups, i):\n",
    "        if i not in groups:\n",
    "            return i\n",
    "        cur = i\n",
    "        while groups[cur] != cur:\n",
    "            cur = groups[cur]\n",
    "        return cur\n",
    "\n",
    "    # How to pick same_thresh:\n",
    "    # 1 degree off: dot product .9998\n",
    "    # 5 degrees off: dot product .996\n",
    "    # 15 degrees off: dot product .966\n",
    "    # x degrees off: dot product = cos(x * pi/180)\n",
    "    dot_thresh = np.cos(same_thresh_degrees * math.pi/180)\n",
    "\n",
    "    # L2 normalize and ensure all vectors are positive \n",
    "    # (or at least as positive as we can make them)\n",
    "    vectors = np.copy(vectors)\n",
    "    for col in range(vectors.shape[1]):\n",
    "        l2 = np.linalg.norm(vectors[:, col])\n",
    "        vectors[:,col] /= l2\n",
    "        if np.sum(vectors[:,col]) < 0:\n",
    "            vectors[:,col] = -vectors[:,col]\n",
    "\n",
    "    # Find all nearly parallel/anti-parallel vectors, probably have to collapse these or we'll have \n",
    "    # extreme numerical instabilities.  \n",
    "    print(\"Basis Shape:\", vectors.shape)\n",
    "    sames = {}\n",
    "    for i in range(vectors.shape[1]):\n",
    "        arr = []\n",
    "        for j in range(vectors.shape[1]):\n",
    "            # TODO: Ack, basis is L1 normalized, needs to be L2 normalized to make the sameness thresh meaningful\n",
    "            dp = np.dot(vectors[:,i], vectors[:,j])\n",
    "            if j > i and dp > dot_thresh or dp < -dot_thresh:\n",
    "                print(\"Bases: \", i, j, \" are nearly identical\")\n",
    "                a = kruskal_top(sames,i)\n",
    "                b = kruskal_top(sames,j)\n",
    "                chosen = min(a,b)\n",
    "                sames[i] = chosen\n",
    "                sames[j] = chosen\n",
    "            arr.append(dp)\n",
    "\n",
    "    final_groups = {}\n",
    "    for i in range(vectors.shape[1]):\n",
    "        if i not in sames:\n",
    "            sames[i] = i\n",
    "        group_id = kruskal_top(sames, i)\n",
    "        if group_id not in final_groups:\n",
    "            final_groups[group_id] = set()\n",
    "        final_groups[group_id].add(i)\n",
    "    \n",
    "    print(final_groups)\n",
    "\n",
    "    out_vecs = []\n",
    "    for group_id in final_groups:\n",
    "        group = final_groups[group_id]\n",
    "        # Build final vector as weighted average of vectors in group\n",
    "        total_vec = np.zeros(vectors.shape[0])\n",
    "        total_weight = 0\n",
    "        for col in group:\n",
    "            total_vec += vectors[:, col] * weights[col]\n",
    "            total_weight += weights[col]\n",
    "        total_vec /= total_weight\n",
    "\n",
    "        # Then L1 normalize output vector\n",
    "        total_vec = total_vec / np.sum(np.abs(total_vec))\n",
    "        out_vecs.append(total_vec)        \n",
    "\n",
    "    return np.stack(out_vecs, axis=-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_widget = widgets.Dropdown(options=list_genera())\n",
    "\n",
    "axis1 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "axis2 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "axis3 = widgets.Dropdown(layout=widgets.Layout(width='50%'))\n",
    "\n",
    "plot = widgets.Button(description='Plot')\n",
    "calculate = widgets.Button(description='Calculate')\n",
    "save = widgets.Button(description='Save')\n",
    "fix = widgets.Button(description='Fix')\n",
    "plot_fix = widgets.Button(description=\"Plot Fix\")\n",
    "plot_fix_resids = widgets.Button(description=\"Plot Fix Resids\")\n",
    "output = widgets.Output()\n",
    "\n",
    "state = {}\n",
    "\n",
    "def filter_df(df, genus):\n",
    "    if genus=='all':\n",
    "        refs_df = list_woltka_refs()\n",
    "    else:\n",
    "        refs_df = list_woltka_refs(genus)\n",
    "    genomes = refs_df['#genome'].tolist()\n",
    "    if len(genomes) == 0 and genus =='all':\n",
    "        print(\"Probably 16S, falling back to raw table\")\n",
    "        filtered_df = df.copy()\n",
    "    else:\n",
    "        filtered_df = df[genomes]\n",
    "        filtered_df_sum = filtered_df.sum(axis=1)\n",
    "        filtered_df = filtered_df[filtered_df_sum >= MIN_GENUS_COUNT]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Define a function that updates the content of y based on what we select for x\n",
    "def update_genus(*args):\n",
    "    genus = genus_widget.value\n",
    "    if genus=='all':\n",
    "        refs_df = list_woltka_refs()\n",
    "    else:\n",
    "        refs_df = list_woltka_refs(genus)\n",
    "    \n",
    "    genomes = refs_df['#genome'].tolist()\n",
    "    species = refs_df['species'].tolist()\n",
    "    choices = []\n",
    "\n",
    "    if genus == 'all' and len(genomes) == 0:\n",
    "        print(\"Probably a 16S table, falling back to sorted columns\")\n",
    "        col_sums = df.sum()\n",
    "        col_sums.name='total'\n",
    "        col_sums = col_sums.sort_values(ascending=False)\n",
    "        genomes=list(col_sums.index)\n",
    "        species=list(genomes)\n",
    "        \n",
    "    for i in range(len(genomes)):\n",
    "        choices.append((species[i] + \"(\" + genomes[i] + \")\", genomes[i]))\n",
    "\n",
    "    axis1.options = choices\n",
    "    axis2.options = choices\n",
    "    axis3.options = choices\n",
    "    axis1.value = genomes[min(0, len(genomes)-1)]\n",
    "    axis2.value = genomes[min(1, len(genomes)-1)]\n",
    "    axis3.value = genomes[min(2, len(genomes)-1)]\n",
    "    \n",
    "    filtered_df = filter_df(df, genus)\n",
    "\n",
    "    state.clear()\n",
    "    state[\"genus\"] = genus\n",
    "    state[\"filtered_df\"] = filtered_df\n",
    "    state[\"cluster_manager\"] = ClusterManager(filtered_df, filtered_df.shape[1])\n",
    "    state[\"breakdown\"] = data_transform.get(genus)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if len(genomes) < 2:\n",
    "            print(genus, \"Not enough reference genomes to do clustering\\n\")\n",
    "            calculate.disabled=True\n",
    "        elif len(filtered_df) <= 10:\n",
    "            print(genus, \"Not enough reads to do clustering\\n\")\n",
    "            calculate.disabled=True\n",
    "        else:\n",
    "            calculate.disabled=False\n",
    "\n",
    "genus_widget.observe(update_genus, names='value')\n",
    "update_genus()\n",
    "\n",
    "display(genus_widget)\n",
    "display(axis1)\n",
    "display(axis2)\n",
    "display(axis3)\n",
    "display(plot)\n",
    "display(calculate)\n",
    "display(save)\n",
    "display(fix)\n",
    "display(output)\n",
    "display(plot_fix)\n",
    "display(plot_fix_resids)\n",
    "\n",
    "def plot_click(button):\n",
    "    print('\"' + state[\"genus\"] + '\":[\"' + axis1.value + '\", \"' + axis2.value + '\", \"' + axis3.value + '\"],')\n",
    "    filtered_df = state[\"filtered_df\"]\n",
    "    plot_scatter3(\n",
    "        state[\"filtered_df\"], \n",
    "        state.get(\"cluster_manager\"), \n",
    "        state[\"genus\"],\n",
    "        axis1.value,\n",
    "        axis2.value,\n",
    "        axis3.value\n",
    "    )\n",
    "    \n",
    "def plot_fix_click(button):\n",
    "    fixed_df = state.get(\"fixed_df\")\n",
    "    if fixed_df is None:\n",
    "        return\n",
    "    plot_scatter3(\n",
    "        fixed_df,\n",
    "        state.get(\"cluster_manager\"), \n",
    "        state[\"genus\"],\n",
    "        fixed_df.columns[0],\n",
    "        fixed_df.columns[min(1, fixed_df.shape[1]-1)],\n",
    "        fixed_df.columns[min(2, fixed_df.shape[1]-1)]\n",
    "    )\n",
    "    \n",
    "def _plot_fix_resids(ax, genus, l1_resids, vlines=None):\n",
    "    # sort the data in ascending order\n",
    "    x = np.sort(l1_resids)\n",
    "\n",
    "    # get the cdf values of y\n",
    "    N = len(l1_resids)\n",
    "    y = np.arange(N) / float(N)\n",
    "\n",
    "    # plotting\n",
    "    ax.set_xlabel('x-axis')\n",
    "    ax.set_ylabel('y-axis')\n",
    "    ax.set_title(genus + 'L1 Residuals CDF')\n",
    "    ax.plot(x, y, marker='o')\n",
    "    ax.axhline(y=0.8, color='gray', linestyle=':')\n",
    "    ax.axhline(y=0.9, color='gray', linestyle=':')\n",
    "    if vlines is not None:\n",
    "        for vline in vlines:\n",
    "            ax.axvline(x=vline, color='gray', linestyle=\":\")\n",
    "        \n",
    "def plot_fix_resids_click(button):\n",
    "    if state.get(\"l1_resids\") is None:\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "    _plot_fix_resids(axs[0], state.get(\"genus\"), state.get(\"l1_resids\"))\n",
    "    _plot_fix_resids(axs[1], state.get(\"genus\"), state.get(\"l1_resid_pcts\"), vlines=[0.05, 0.10])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def _calculate(to_cluster, genus):\n",
    "    num_dims = 10 # Beware, this is completely dataset dependent!\n",
    "    if genus != 'all':\n",
    "        num_dims = to_cluster.shape[1]\n",
    "    cm = ClusterManager(to_cluster, num_dims)\n",
    "    rc = RecursiveClusterer()\n",
    "    rc.run(cm)\n",
    "    return {\"cluster_manager\": cm, \"clusterer\": rc}\n",
    "\n",
    "def calculate_click(button):\n",
    "    updated = _calculate(state[\"filtered_df\"], state[\"genus\"])\n",
    "    state.update(updated)\n",
    "    \n",
    "def _save(genus, filtered_df, cm, data_path):\n",
    "    if cm == None:\n",
    "        print(\"No clusters to save\")\n",
    "        return\n",
    "\n",
    "    cs = cm.cluster_state\n",
    "    subspace_bases = calc_subspace_bases(filtered_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "\n",
    "    surfaces = []\n",
    "    for key in subspace_bases:\n",
    "        basis = subspace_bases[key]\n",
    "        basis_df = pd.DataFrame(basis, index=filtered_df.columns)\n",
    "        print(basis_df)\n",
    "        surfaces.append(basis_df.to_json())\n",
    "    \n",
    "    data_transform[genus] = surfaces\n",
    "    only_genus = {genus: surfaces}\n",
    "\n",
    "    with open(data_path, 'w') as outfile:\n",
    "        print(\"Saving to: \" + data_path)\n",
    "        json.dump(only_genus, outfile)\n",
    "    \n",
    "def save_click(button):\n",
    "    _save(state[\"genus\"], state[\"filtered_df\"], state[\"cluster_manager\"], DATA_TRANSFORM_PATH)\n",
    "\n",
    "def _fix(cm, filtered_df):\n",
    "    if cm is None:\n",
    "        print(\"No clusters to fix!\")\n",
    "        return {}\n",
    "    cs = cm.cluster_state\n",
    "    subspace_bases = calc_subspace_bases(filtered_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "    cluster_counts = cs.get_cluster_counts()\n",
    "    \n",
    "    if len(subspace_bases) == 0:\n",
    "        print(\"Could not identify species vectors\")\n",
    "        return {}\n",
    "    # L1 normalize subspace bases.\n",
    "    for c_id in subspace_bases:\n",
    "        for col in range(subspace_bases[c_id].shape[1]):\n",
    "            l1_len = np.sum(np.abs(subspace_bases[c_id][:,col]))\n",
    "            if l1_len != 0:\n",
    "                subspace_bases[c_id][:,col] = subspace_bases[c_id][:,col] / l1_len\n",
    "            if np.sum(subspace_bases[c_id][:,col]) < 0:\n",
    "                subspace_bases[c_id][:,col] = -subspace_bases[c_id][:,col]\n",
    "\n",
    "    # TODO FIXME HACK:  Find a way to include SVs from all dims of higher dim clusters rather than just the first\n",
    "    full_basis = np.stack([subspace_bases[c][:,0] for c in subspace_bases], axis=1)\n",
    "    basis_counts = [cluster_counts[c] for c in subspace_bases]\n",
    "    full_basis = merge_similar_vectors(full_basis, basis_counts, same_thresh_degrees=5)\n",
    "    \n",
    "    out_pts = []\n",
    "    l1_resids = []\n",
    "    l1_resid_pcts = []\n",
    "    max_resid = 0\n",
    "    max_resid_pct = 0\n",
    "    for i in range(filtered_df.shape[0]):\n",
    "        pt = filtered_df.iloc[i].T.to_numpy()\n",
    "        output_pt, l2_resid = scipy.optimize.nnls(full_basis, pt)\n",
    "        nnls_proj = np.matmul(full_basis, output_pt)\n",
    "        l1_resid = np.sum(np.abs(pt - nnls_proj))\n",
    "\n",
    "        output_pt = pd.DataFrame(output_pt).T\n",
    "        output_pt.index = [filtered_df.index[i]]\n",
    "\n",
    "        l1_resids.append(l1_resid)\n",
    "        length = np.sum(np.abs(pt))\n",
    "        l1_resid_pct = l1_resid / length\n",
    "        l1_resid_pcts.append(l1_resid_pct)\n",
    "        \n",
    "            \n",
    "        max_resid = max(max_resid, abs(l1_resid))\n",
    "        if pt.sum() > 0: \n",
    "            max_resid_pct = max(max_resid_pct, abs(l1_resid)/pt.sum())\n",
    "\n",
    "        out_pts.append(output_pt)\n",
    "    \n",
    "    output_df = pd.concat(out_pts)\n",
    "    output_df = output_df.fillna(0)\n",
    "    output_df.columns = list([\"SV\"+str(i) for i in range(len(output_df.columns))])\n",
    "    \n",
    "    return {\"fixed_df\": output_df, \"l1_resids\": l1_resids, \"l1_resid_pcts\": l1_resid_pcts}\n",
    "    \n",
    "def fix_click(button):\n",
    "    updated = _fix(state.get(\"cluster_manager\"), state[\"filtered_df\"])\n",
    "    state.update(updated)\n",
    "    \n",
    "plot.on_click(plot_click)\n",
    "calculate.on_click(calculate_click)\n",
    "save.on_click(save_click)\n",
    "fix.on_click(fix_click)\n",
    "plot_fix.on_click(plot_fix_click)\n",
    "plot_fix_resids.on_click(plot_fix_resids_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eff84c-b23b-4acc-b8c0-8a0213c053ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_calculate(prefix):\n",
    "    genera = list_genera()\n",
    "    \n",
    "    for genus in genera:\n",
    "        if genus == \"all\":\n",
    "            continue\n",
    "        print(\"Genus:\", genus)\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        \n",
    "        ns = _calculate(filtered_df, genus)\n",
    "        cm = ns[\"cluster_manager\"]\n",
    "        \n",
    "        ns = _fix(cm, filtered_df)\n",
    "        if len(ns) == 0:\n",
    "            # no species vectors\n",
    "            print(\"SKIP\")\n",
    "            continue\n",
    "            \n",
    "        fixed_df = ns[\"fixed_df\"]\n",
    "        l1_resids = ns[\"l1_resids\"]\n",
    "        l1_resid_pcts = ns[\"l1_resid_pcts\"]\n",
    "        \n",
    "        # fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "        # _plot_fix_resids(axs[0], genus, l1_resids)\n",
    "        # _plot_fix_resids(axs[1], genus, l1_resid_pcts, vlines=[0.05, 0.10])\n",
    "        # plt.show()\n",
    "        \n",
    "        _save(genus, filtered_df, cm, \"./results/\" + prefix + \"/\" + genus + \".json\")\n",
    "# bulk_calculate(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d952d-6ae4-45f8-9831-13880bd74998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_compare(prefix):\n",
    "    genera = list_genera()\n",
    "    \n",
    "    calculation_failed = 0\n",
    "    calculation_success = 0\n",
    "    calculation_bad_result = 0\n",
    "    increased_dim = 0\n",
    "    columns_evaluated = 0\n",
    "    columns_dropped = 0\n",
    "\n",
    "    for genus in genera:\n",
    "        if genus == \"all\":\n",
    "            continue\n",
    "        print(\"Genus:\", genus)\n",
    "        file = \"./results/\" + prefix + \"/\" + genus + \".json\"\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        \n",
    "        initial_dim = filtered_df.shape[1]\n",
    "        columns_evaluated += initial_dim\n",
    "        \n",
    "        if not os.path.exists(file):\n",
    "            print(\"File does not exist, calculation failed.\")\n",
    "            calculation_failed += 1\n",
    "            continue\n",
    "        \n",
    "        cm = ClusterManager.apply_bases_from_file(file, filtered_df)\n",
    "        cm.finalize()\n",
    "        \n",
    "        ns = _fix(cm, filtered_df)\n",
    "        if len(ns) == 0:\n",
    "            # no species vectors\n",
    "            print(\"No Species Vectors Were Calculated\")\n",
    "            calculation_failed += 1\n",
    "            continue\n",
    "            \n",
    "        fixed_df = ns[\"fixed_df\"]\n",
    "        l1_resids = ns[\"l1_resids\"]\n",
    "        l1_resid_pcts = ns[\"l1_resid_pcts\"]\n",
    "        \n",
    "        x = np.sort(l1_resid_pcts)\n",
    "        x_80 = l1_resid_pcts[int(.8 * len(l1_resid_pcts))]\n",
    "        x_90 = l1_resid_pcts[int(.9 * len(l1_resid_pcts))]\n",
    "        \n",
    "        print(x_80, x_90)\n",
    "        \n",
    "        final_dim = fixed_df.shape[1]\n",
    "                \n",
    "        if x_80 < 0.05 and final_dim <= initial_dim:\n",
    "            print(\"Success\")\n",
    "            calculation_success += 1\n",
    "            print(\"Dropping\", initial_dim - final_dim, \"columns.\")\n",
    "            columns_dropped += initial_dim - final_dim\n",
    "        elif x_80 >= 0.05:\n",
    "            print(\"Bad Result\")\n",
    "            calculation_bad_result += 1\n",
    "        elif final_dim > initial_dim:\n",
    "            increased_dim += 1        \n",
    "        \n",
    "        # fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "        # _plot_fix_resids(axs[0], genus, l1_resids)\n",
    "        # _plot_fix_resids(axs[1], genus, l1_resid_pcts, vlines=[0.05, 0.10])\n",
    "        # plt.show()\n",
    "    \n",
    "    print(\"Success\", calculation_success)    \n",
    "    print(\"Fail\", calculation_failed)\n",
    "    print(\"Bad Result\", calculation_bad_result)\n",
    "    print(\"Bad Result (Increased Dim)\", increased_dim)\n",
    "    print(\"\\nTotal Dimensionality Reduction:\", columns_dropped, str(int((columns_dropped / columns_evaluated) * 100)) + \"%\")\n",
    "    \n",
    "# bulk_compare(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb7f70-0a9d-4fd8-a571-ae4402428fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_run_click(button):\n",
    "    cmd = cluster_command.value\n",
    "    ss = cmd.split()\n",
    "    f_df = state[\"filtered_df\"]\n",
    "    cm = state[\"cluster_manager\"]\n",
    "    cs = cm.cluster_state\n",
    "\n",
    "    if ss[0] == 'split':\n",
    "        cluster_id = int(ss[1])\n",
    "        cluster_pieces = int(ss[2])\n",
    "        cm.get_split_cluster(cluster_id, cluster_pieces).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'merge':\n",
    "        cluster_id_a = int(ss[1])\n",
    "        cluster_id_b = int(ss[2])\n",
    "        final_dim = int(ss[3])\n",
    "        cm.get_merge_clusters(cluster_id_a, cluster_id_b, final_dim).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'delete' or ss[0] == 'del':\n",
    "        cluster_id = int(ss[1])\n",
    "        cm.get_delete_cluster(cluster_id).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'reassign':\n",
    "        cm.get_reassign_nearest(dim_penalty=10, outlier_thresh=0.10).apply()\n",
    "        cm.finalize()\n",
    "    elif ss[0] == 'prevalence':\n",
    "        print(df.shape)\n",
    "        print(f_df.shape)\n",
    "        print(\"Approximate \" + state[\"genus\"] + \" prevalence\")\n",
    "        print(str((f_df.shape[0] / df.shape[0]) * 100) + \"%\")\n",
    "        threshold_df = counts_to_presence_absence(df[f_df.columns], 500)\n",
    "        pairwise_eval(threshold_df)\n",
    "    elif ss[0] == 'info':\n",
    "        \n",
    "        if len(ss) >= 2:\n",
    "            cluster_id = int(ss[1])\n",
    "            to_run = [cluster_id]\n",
    "        else:\n",
    "            to_run = range(cm.max_cluster_id())\n",
    "        \n",
    "        for cluster_id in to_run:\n",
    "            idx = cs.clusters == cluster_id\n",
    "\n",
    "            # print(\"Samples In Cluster:\")\n",
    "            # print(f_df.iloc[idx].index)\n",
    "\n",
    "            subspace_bases = calc_subspace_bases(f_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "            basis = subspace_bases[cluster_id]\n",
    "            basis_df = pd.DataFrame(basis, index=f_df.columns)\n",
    "\n",
    "            abs_basis = basis_df.abs()\n",
    "            subspace = abs_basis.idxmax()\n",
    "\n",
    "            pdf = pd.DataFrame(data=subspace, columns=[\"#genome\"])\n",
    "            pdf = pdf.merge(woltka_meta_df, on=\"#genome\")\n",
    "\n",
    "            print(\"Approximate Name(s)\", cluster_id)\n",
    "            print(pdf[[\"#genome\", \"species\"]])\n",
    "    elif ss[0] == 'diff':\n",
    "        first = int(ss[1])\n",
    "        second = int(ss[2])\n",
    "        \n",
    "        subspace_bases = calc_subspace_bases(f_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "        basis_1 = subspace_bases[first]\n",
    "        basis_2 = subspace_bases[second]\n",
    "        \n",
    "        b1 = basis_1[:,0]\n",
    "        b2 = basis_2[:,0]\n",
    "        \n",
    "        if np.sum(b1) < 0:\n",
    "            b1 = -b1\n",
    "        if np.sum(b2) < 0:\n",
    "            b2 = -b2\n",
    "        \n",
    "        b1_l2norm = np.linalg.norm(b1)\n",
    "        b2_l2norm = np.linalg.norm(b2)\n",
    "        \n",
    "        b1 = b1 / b1_l2norm\n",
    "        b2 = b2 / b2_l2norm\n",
    "        \n",
    "        dot = np.dot(b1,b2)\n",
    "        theta = np.arccos(dot)\n",
    "        print(\"Dot Product:\", dot, \"Diff Angle Theta (Degrees):\", theta * 180/math.pi)        \n",
    "        diffs = np.abs(b2 - b1)\n",
    "        max_diff = np.argmax(diffs)\n",
    "        print(f_df.columns[max_diff],diffs[max_diff], b1[max_diff], b2[max_diff])\n",
    "    elif ss[0] == 'taxi':\n",
    "        new_filt_df = f_df.copy()\n",
    "\n",
    "        for cluster_id in cs.cluster_dims:\n",
    "            idx = cs.clusters == cluster_id\n",
    "        \n",
    "        cluster_dim = cs.cluster_dims[cluster_id]\n",
    "        if cluster_dim == 1:\n",
    "            new_filt_df[\"AX\"+str(cluster_id)]\n",
    "    elif ss[0] == \"load\":\n",
    "        f = ss[1]\n",
    "        # ./results/celeste_ecoli_many.json\n",
    "        # TODO FIXME HACK:  Should I apply it to df or filtered df?\n",
    "        cm = ClusterManager.apply_bases_from_file(f, f_df)\n",
    "        cm.finalize()\n",
    "        state[\"cluster_manager\"] = cm\n",
    "    else:\n",
    "        print(\"Unsupported Command\")\n",
    "\n",
    "cluster_command = widgets.Text(layout=widgets.Layout(width='80%'), placeholder='split 1 2')\n",
    "cluster_run = widgets.Button(description='Run Command')\n",
    "\n",
    "display(cluster_command)\n",
    "display(cluster_run)\n",
    "\n",
    "cluster_run.on_click(cluster_run_click)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a3879-ff0d-4342-a9a1-6b81f5175528",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"cluster_manager\"].cluster_state.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400498b-2641-4b4d-bd37-b478de01b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_plot(df, genlist=None, suptitle=None):\n",
    "    if genlist is None:\n",
    "        genlist = list_genera()\n",
    "    print(len(genlist))\n",
    "    i = 0\n",
    "    fig = None\n",
    "    passing = 0\n",
    "    for genus in genlist:\n",
    "        if genus == 'all':\n",
    "            continue\n",
    "\n",
    "        filtered_df = filter_df(df, genus)\n",
    "        sample_pass = round((filtered_df.shape[0] / df.shape[0]) * 100,2)\n",
    "        print(genus + \", \" + str(sample_pass) + \"%\" + \", \" + imsms_qualitative_class[genus][0])\n",
    "\n",
    "        # if sample_pass < 15:\n",
    "        #     passing += 1\n",
    "        #     continue\n",
    "        \n",
    "        if i == 0:\n",
    "            fig = plt.figure()\n",
    "            if suptitle is not None:\n",
    "                fig.suptitle(suptitle)\n",
    "        ax = fig.add_subplot(2, 3, i+1, projection='3d')\n",
    "        i += 1\n",
    "\n",
    "        classification = \"\\n(Co-Exclusive)\"\n",
    "        if imsms_qualitative_class[genus][0] != \"YES\":\n",
    "            classification = \"\\n(Co-Occurring)\"\n",
    "        plot_scatter3(\n",
    "            filtered_df, \n",
    "            ClusterManager(filtered_df, filtered_df.shape[1]), \n",
    "            genus + \": \" + str(sample_pass) + \"%\" + classification,\n",
    "            imsms_plots[genus][0],\n",
    "            imsms_plots[genus][1],\n",
    "            imsms_plots[genus][2],\n",
    "            subplot_ax = ax\n",
    "        )\n",
    "        \n",
    "        if i == 6:\n",
    "            i = 0\n",
    "            plt.show()\n",
    "    print(passing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81fe26-61de-4124-8259-28a4c65373dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imsms = BiomTable(\"./dataset/biom/imsms-combined-none.biom\").load_dataframe()\n",
    "# df_finrisk = BiomTable(\"./dataset/biom/finrisk-combined-none.biom\").load_dataframe()\n",
    "# df_sol = BiomTable(\"./dataset/biom/sol_public_99006-none.biom\").load_dataframe()\n",
    "\n",
    "#bulk_plot(df)\n",
    "\n",
    "# bulk_plot(df_imsms, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"iMSMS\")\n",
    "# bulk_plot(df_finrisk, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"FINRISK\")\n",
    "# bulk_plot(df_sol, [\"Akkermansia\", \"Butyricicoccus\", \"Dialister\", \"Eggerthella\", \"Methanobrevibacter\", \"Ruminiclostridium\"], \"SOL\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d682dd8-b8ec-49e8-8dc2-3b8e8af61a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"filtered_df\"].sum(axis=1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222429f-23fe-4a65-9736-7f31913a0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = state[\"cluster_manager\"]\n",
    "\n",
    "full_clusters = cm.apply_active_clustering(df, dim_penalty=5, outlier_thresh=.1)\n",
    "print(full_clusters.value_counts())\n",
    "\n",
    "full_df = df[cm.data_cols]\n",
    "full_cm = ClusterManager(full_df, len(cm.data_cols))\n",
    "full_cm.cluster_state = cm.cluster_state.copy()\n",
    "full_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "\n",
    "# plot_scatter3(full_df, full_cm, \"TEST\", full_df.columns[0],full_df.columns[1],full_df.columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d47c32-e568-4331-96f6-542299994d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my next trick, I will convert linear subspaces to conic polyhedrons\n",
    "# Since its not entirely clear how to do this when there is noise, we will use the following idea:\n",
    "# Points on a 1D linear subspace results in an obvious basis vector\n",
    "# Points on a 2D linear subspace correspond to a 1D line segment on the simplex.  The endpoints of that line\n",
    "# segment result in two basis vectors\n",
    "# Points on a 3D linear subspace correspond to a 2D polygon on the simplex.  The \"best\" simplification of\n",
    "# that polygon to a triangle results in three basis vectors.  The best simplification is likely the\n",
    "# triangle with the largest area, though we can try other algorithms.  \n",
    "# For a 4D, it should result in a 3D polyhedron on the simplex, the best simplification of that to a \n",
    "# tetrahedron is the conic polyhedron.  \n",
    "# For a 5D, it should result in a 4-simplex on the the 5D simplex.  Grarghh.  \n",
    "\n",
    "# NOTE:  Finding the largest n simplex in an n-1 dimensional convex hull is NP hard and inapproximable.\n",
    "# RIP.  https://stackoverflow.com/questions/50049658/largest-simplex-in-convex-hull-of-points-in-n-dimensions\n",
    "# We will go ahead and use either brute force or some bs heuristic.  \n",
    "cm = state[\"cluster_manager\"]\n",
    "cs = cm.cluster_state\n",
    "filtered_df = state[\"filtered_df\"]\n",
    "subspace_bases = calc_subspace_bases(filtered_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "all_proj = calc_projected(filtered_df, cs.clusters, subspace_bases)\n",
    "\n",
    "conic_bases = {}\n",
    "for cluster_id in subspace_bases:\n",
    "    if subspace_bases[cluster_id].shape[1] == 1:\n",
    "        conic_bases[cluster_id] = subspace_bases[cluster_id]\n",
    "    else:\n",
    "        print(\"Cluster ID:\", cluster_id, \"Dim:\", cs.cluster_dims[cluster_id])\n",
    "        \n",
    "        # Grab points in the cluster\n",
    "        idx = cs.clusters == cluster_id\n",
    "        cluster_proj = all_proj[:,idx]\n",
    "        print(cluster_proj.shape)\n",
    "        \n",
    "        # Push them out to the simplex (L1 normalize them)\n",
    "        for i in range(cluster_proj.shape[1]):\n",
    "            c_pt = cluster_proj[:,i]\n",
    "            if c_pt.sum() != 0:\n",
    "                c_pt = c_pt/c_pt.sum()\n",
    "            cluster_proj[:,i] = c_pt\n",
    "        \n",
    "        M = subspace_bases[cluster_id]\n",
    "        cluster_proj_low_d = np.matmul(M.T, cluster_proj).T        \n",
    "        \n",
    "        # Since we've pushed everything to the simplex, qhull's convex hull will say we are too\n",
    "        # low dimension.  We add a point at the origin to get it to give us a clean set of points.\n",
    "        \n",
    "        cluster_proj_low_d = np.vstack([cluster_proj_low_d, np.zeros(cluster_proj_low_d.shape[1])])\n",
    "        print(cluster_proj_low_d.shape)\n",
    "        hull = scipy.spatial.ConvexHull(cluster_proj_low_d)\n",
    "        \n",
    "        print(hull.vertices)\n",
    "        hull_verts = hull.vertices[hull.vertices != cluster_proj_low_d.shape[0]-1]\n",
    "        print(hull_verts)\n",
    "        plt.plot(cluster_proj_low_d[hull_verts,0], cluster_proj_low_d[hull_verts,1], 'ro')\n",
    "        for simplex in hull.simplices:\n",
    "            plt.plot(cluster_proj_low_d[simplex, 0], cluster_proj_low_d[simplex, 1], 'k-')\n",
    "        plt.show()\n",
    "        \n",
    "        if subspace_bases[cluster_id].shape[1] == 2:\n",
    "            if len(hull_verts) != 2:\n",
    "                print(\"Dangit hull verts\")\n",
    "                print(hull_verts)\n",
    "                print(all_proj[:,idx][:, hull_verts])\n",
    "                raise Exception(\"Aww I thought it was working, check coplanar points in convex hull?\")\n",
    "            \n",
    "            idx = cs.clusters == cluster_id\n",
    "            cluster_proj = all_proj[:,idx]\n",
    "            \n",
    "            conic_bases[cluster_id] = cluster_proj[:, hull_verts]\n",
    "            print(conic_bases[cluster_id])\n",
    "            print(subspace_bases[cluster_id])\n",
    "            \n",
    "        if subspace_bases[cluster_id].shape[1] == 3:\n",
    "            # Ugh.  Find largest triangle in a convex hull.  At least its more plausible in 3D than 4+\n",
    "            # A possible optimization for this case is here:\n",
    "            # https://stackoverflow.com/questions/1621364/how-to-find-largest-triangle-in-convex-hull-aside-from-brute-force-search/1621913#1621913\n",
    "            # Area of triangle by heron's formula\n",
    "            \n",
    "            largest_area_idx = [-1,-1,-1]\n",
    "            largest_area_sq = 0\n",
    "            for i in hull_verts:\n",
    "                for j in hull_verts:\n",
    "                    for k in hull_verts:\n",
    "                        pt_i = cluster_proj[:,i]\n",
    "                        pt_j = cluster_proj[:,j]\n",
    "                        pt_k = cluster_proj[:,k]\n",
    "                        a = np.linalg.norm(pt_j - pt_i)\n",
    "                        b = np.linalg.norm(pt_k - pt_j)\n",
    "                        c = np.linalg.norm(pt_i - pt_k)\n",
    "                        p = (a+b+c)/2\n",
    "                        area_sq = p * (p-a) * (p-b) * (p-c)\n",
    "                        if area_sq > largest_area_sq:\n",
    "                            largest_area_idx = [i,j,k]\n",
    "                            largest_area_sq = area_sq\n",
    "            \n",
    "            # conic_bases[cluster_id] = cluster_proj[:, largest_area_idx]\n",
    "            conic_bases[cluster_id] = cluster_proj[:, hull_verts]\n",
    "            print(conic_bases[cluster_id])\n",
    "            print(subspace_bases[cluster_id])\n",
    "            \n",
    "c1 = min(3, len(filtered_df.columns)-1)\n",
    "c2 = min(1, len(filtered_df.columns)-1)\n",
    "c3 = min(4, len(filtered_df.columns)-1)\n",
    "plot_scatter3(filtered_df, cm, \"CONIC TEST\", filtered_df.columns[c1],filtered_df.columns[c2],filtered_df.columns[c3], target_cluster=7, conic_bases=conic_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8c33c-5b14-413a-b181-8959eb7741a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to transform to a linear subspace, not a conic polyhedron, ugh...\n",
    "# =()^(1) * \n",
    "# See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n",
    "# for a way to project onto a conic polyhedron instead\n",
    "# (of course, we still need to compute the conic polyhedron rather than just the subspaces. Ugh.)\n",
    "cm = state[\"cluster_manager\"]\n",
    "cs = cm.cluster_state\n",
    "filtered_df = state[\"filtered_df\"]\n",
    "\n",
    "#Blah, should make this an enum\n",
    "USE_REFERENCE_BASIS = True\n",
    "USE_CALCULATED_BASIS = False\n",
    "\n",
    "if USE_REFERENCE_BASIS:\n",
    "    full_basis = (all_df.copy() / all_df.sum(axis=0)).T.to_numpy()\n",
    "elif USE_CALCULATED_BASIS:\n",
    "    subspace_bases = calc_subspace_bases(filtered_df.T.to_numpy(), cs.clusters, cs.cluster_dims)\n",
    "\n",
    "    for c_id in subspace_bases:\n",
    "        for col in range(subspace_bases[c_id].shape[1]):\n",
    "            # If we are to maintain read counts in the transformed space, basis vectors must be L1 normalized\n",
    "            # This also negates ones which point in overall negative directions.  \n",
    "            l1_len = np.sum(subspace_bases[c_id][:,col])\n",
    "            if l1_len != 0:\n",
    "                subspace_bases[c_id][:,col] = subspace_bases[c_id][:,col] / l1_len\n",
    "\n",
    "    print(subspace_bases)\n",
    "    full_basis = np.concatenate([subspace_bases[c] for c in subspace_bases], axis=1)\n",
    "\n",
    "print(full_basis)\n",
    "\n",
    "# Find all nearly parallel/anti-parallel vectors, probably have to collapse these or we'll have \n",
    "# extreme numerical instabilities.  \n",
    "SAMENESS_THRESH = 0.9\n",
    "print(\"Basis Shape:\", full_basis.shape)\n",
    "sames = {}\n",
    "for i in range(full_basis.shape[1]):\n",
    "    arr = []\n",
    "    for j in range(full_basis.shape[1]):\n",
    "        # TODO: Ack, basis is L1 normalized, needs to be L2 normalized to make the sameness thresh meaningful\n",
    "        dp = np.dot(full_basis[:,i], full_basis[:,j])\n",
    "        if j > i and dp > SAMENESS_THRESH or dp < -SAMENESS_THRESH:\n",
    "            print(\"Bases: \", i, j, \" are nearly identical\")\n",
    "            # TODO: If there are 3+ axes that are the same, this code won't work right.\n",
    "            sames[j] = i\n",
    "        arr.append(dp)\n",
    "    # print(arr)\n",
    "    \n",
    "# Maybe would be better to do a weighted average of near identical vectors, but for now,\n",
    "# we'll just take the first one, since we are always more confident in vectors towards the beginning of\n",
    "# the list based on the lower dimension and increased number of samples.  \n",
    "for same_key in sames:\n",
    "    full_basis[:, same_key] = full_basis[:, sames[same_key]]\n",
    "\n",
    "vec_index = 0\n",
    "for cluster_id in subspace_bases:\n",
    "    for col in range(subspace_bases[cluster_id].shape[1]):\n",
    "        if vec_index in sames:\n",
    "            subspace_bases[cluster_id][:, col] = full_basis[:, sames[vec_index]]\n",
    "        vec_index += 1\n",
    "\n",
    "# Unclear what to do with outlier points which are either unknown species vectors or rare co-occurrence of\n",
    "# identified species vectors.  We can build a full basis of the identified species vectors to account for\n",
    "# co-occurrence, but our basis vectors for each subspace are not orthogonal to each other\n",
    "# This can result in underconstrained solutions and numerical instability.  If we use Graham-Schmidt \n",
    "# orthonormalization or something similar on our basis vectors in the order we wish to assign them \n",
    "# weight, (and our spaces are sorted by number of dimensions and number of points, so we are assigning\n",
    "# most common first), that may break ties in a clear way.  Majority of numerical instability probably\n",
    "# comes from having multiple estimates of the same species vector, (A 1d vector and a 2d surface overlapping\n",
    "# would do that, as would two 2d surfaces that intersect)\n",
    "solver_cols = {}\n",
    "Ms = {}\n",
    "\n",
    "M = full_basis\n",
    "keep_cols = [i for i in range(full_basis.shape[1]) if i not in sames]\n",
    "M = M[:, keep_cols]\n",
    "cols = [\"ax\" + str(c) for c in keep_cols]\n",
    "\n",
    "solver_cols[OUTLIER_CLUSTER_ID] = cols\n",
    "Ms[OUTLIER_CLUSTER_ID] = M\n",
    "tot_vecs = 0\n",
    "for cluster_id in subspace_bases:\n",
    "    M_i = subspace_bases[cluster_id]\n",
    "    Ms[cluster_id] = M_i\n",
    "    solver_cols[cluster_id] = []\n",
    "    for i in range(subspace_bases[cluster_id].shape[1]):\n",
    "        if tot_vecs in sames:\n",
    "            solver_cols[cluster_id].append(\"ax\" + str(sames[tot_vecs]))\n",
    "        else:\n",
    "            solver_cols[cluster_id].append(\"ax\" + str(tot_vecs))\n",
    "        tot_vecs += 1\n",
    "\n",
    "# For points which do cluster well, we have orthonormal bases (though we want to switch to non orthogonal\n",
    "# bases of conic polyhedra)\n",
    "to_transform = df\n",
    "full_clusters = cm.apply_active_clustering(to_transform, dim_penalty=5, outlier_thresh=.1)\n",
    "print(full_clusters.value_counts())\n",
    "\n",
    "full_df = to_transform[cm.data_cols]\n",
    "full_cm = ClusterManager(full_df, len(cm.data_cols))\n",
    "full_cm.cluster_state = cm.cluster_state.copy()\n",
    "full_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "\n",
    "out_pts = []\n",
    "max_resid = 0\n",
    "for i in range(to_transform.shape[0]):\n",
    "    pt = to_transform[cm.data_cols].iloc[i].T.to_numpy()\n",
    "    cluster_assignment = full_clusters.iloc[i]\n",
    "    \n",
    "    output_pt_lin_subspace = np.matmul(solvers[cluster_assignment], pt)\n",
    "    output_pt_nnls, l2_resid = scipy.optimize.nnls(Ms[cluster_assignment], pt)\n",
    "    nnls_proj = np.matmul(Ms[cluster_assignment], output_pt_nnls)\n",
    "    l1_resid = np.sum(pt - nnls_proj)\n",
    "    # print(output_pt_lin_subspace, \" vs \", output_pt_nnls, \"(\", resid, \")\")\n",
    "    output_pt = output_pt_nnls                                                   \n",
    "                                                   \n",
    "    output_pt = pd.DataFrame(output_pt).T\n",
    "    output_pt.index = [to_transform.index[i]]\n",
    "    output_pt.columns = solver_cols[cluster_assignment]\n",
    "    # print(pt.sum(), output_pt.sum(axis=1), \"+\", l1_resid)\n",
    "    max_resid = max(max_resid, abs(l1_resid))\n",
    "    if pt.sum() > 0: \n",
    "        max_resid_pct = max(max_resid_pct, abs(l1_resid)/pt.sum())\n",
    "    \n",
    "    out_pts.append(output_pt)\n",
    "\n",
    "print(\"MAX RESIDUAL: \", max_resid)\n",
    "    \n",
    "output_df = pd.concat(out_pts)\n",
    "output_df = output_df.fillna(0)\n",
    "print(output_df)\n",
    "print(output_df.loc[\"S.71801.0073.4.7.17\"])    \n",
    "output_cm = ClusterManager(output_df, output_df.shape[1])\n",
    "output_cm.cluster_state = cm.cluster_state.copy()\n",
    "output_cm.cluster_state.clusters = full_clusters.to_numpy()\n",
    "output_cm.cluster_state.cluster_dims[-1] = output_df.shape[1]\n",
    "\n",
    "c1 = min(0, len(output_df.columns)-1)\n",
    "c2 = min(1, len(output_df.columns)-1)\n",
    "c3 = min(2, len(output_df.columns)-1)\n",
    "plot_scatter3(output_df, full_cm, \"TEST\", output_df.columns[c1],output_df.columns[c2],output_df.columns[c3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8a56-65ac-4b82-b352-408b45646641",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_df.head())\n",
    "c1 = min(0, len(output_df.columns)-1)\n",
    "c2 = min(3, len(output_df.columns)-1)\n",
    "c3 = min(4, len(output_df.columns)-1)\n",
    "plot_scatter3(output_df, full_cm, \"TEST\", output_df.columns[c1],output_df.columns[c2],output_df.columns[c3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae035ee-0eaf-4341-b916-4f446c0c9fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fead6-40cb-41aa-abec-97c050ceb608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a57b8-d12e-44fb-b384-a4e022fdbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = np.array([[1,1],[5,5],[3,0],[2,1]]) \n",
    "\n",
    "hull = scipy.spatial.ConvexHull(pts)\n",
    "\n",
    "plt.plot(pts[:,0], pts[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-')\n",
    "    \n",
    "# plt.plot(pts[hull.vertices,0], pts[hull.vertices,1], 'r--', lw=2)\n",
    "# plt.plot(pts[hull.vertices[0],0], pts[hull.vertices[0],1], 'ro')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ec395-0f49-4e84-be96-73bfdd207d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = state[\"filtered_df\"]\n",
    "plot_scatter3(\n",
    "    state[\"filtered_df\"], \n",
    "    state.get(\"cluster_manager\"), \n",
    "    state[\"genus\"],\n",
    "    axis1.value,\n",
    "    axis2.value,\n",
    "    axis3.value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45ab5e-ecf7-4924-9308-f4a4d1446e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/celeste_ecoli_many.json\") as infile:\n",
    "    data_transform = json.load(infile)\n",
    "    all_bases = []\n",
    "    counter = 0\n",
    "    for subspace in range(len(data_transform[\"all\"])):\n",
    "        conic_basis = pd.read_json(data_transform[\"all\"][subspace])\n",
    "        conic_basis.columns = range(counter, counter + conic_basis.shape[1])\n",
    "        counter += conic_basis.shape[1]\n",
    "        all_bases.append(conic_basis)\n",
    "    conic_basis = pd.concat(all_bases, axis=1)\n",
    "\n",
    "\n",
    "# foo_basis = pd.DataFrame([[100/600, 200/700],[100/600, 0],[400/600, 500/700]])\n",
    "# foo_df = pd.DataFrame([[100, 400],[100, 0],[400, 1000]])\n",
    "\n",
    "# print(transformer.transform(foo_df, foo_basis))\n",
    "\n",
    "\n",
    "# conic_basis = transformer.L1_normalize(conic_basis)\n",
    "# print(conic_basis.loc[[\"G000183345\", \"G000026345\",\"G000026325\", \"G000299455\", \"G000008865\", \"G001283625\", \"G000759795\", \"G001941055\", \"G000009065\"],:])\n",
    "transformed = transformer.transform(df, conic_basis)\n",
    "colors = []\n",
    "for i in transformed.index:\n",
    "    ss = i.split(\".\")\n",
    "    if ss[1].startswith(\"4\"):\n",
    "        colors.append(\"m\")\n",
    "    else:\n",
    "        colors.append(\"b\")\n",
    "\n",
    "plt.scatter(transformed[0], transformed[1], c=colors)\n",
    "plt.show()\n",
    "# plt.scatter(df[\"G000183345\"], df[\"G000026345\"])\n",
    "# plt.show()\n",
    "# plt.scatter(transformed[0], transformed[1])\n",
    "# plt.show()\n",
    "\n",
    "transformed.hist(\"L1_resid\")\n",
    "plt.show()\n",
    "transformed[transformed[\"L1_resid\"] > 50000].hist(\"L1_resid\")\n",
    "plt.show()\n",
    "\n",
    "transformed = transformed.sort_values(\"L1_resid\", ascending=False)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(transformed.head(n=100)[[\"L1_resid\", \"WorstAxis\"]])\n",
    "    \n",
    "foo_df = df.loc[transformed.head(n=200).index]\n",
    "\n",
    "# plot_scatter3(\n",
    "#     foo_df, \n",
    "#     ClusterManager(foo_df, 1),\n",
    "#     state[\"genus\"],\n",
    "#     axis1.value,\n",
    "#     axis2.value,\n",
    "#     axis3.value\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dbd47-1f47-4095-a2d5-6fbd00295c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f79b2-369d-4fbf-b270-e5c4fcec1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra junk for plotting info about the bacteroides samples\n",
    "# bacteroides_sample_info = CSVTable(\"./dataset/biom/14360_20211220-082840.txt\", delimiter=\"\\t\")\n",
    "# bacteroides_sample_info = bacteroides_sample_info.load_dataframe()\n",
    "# bacteroides_sample_info.index = bacteroides_sample_info[\"sample_name\"]\n",
    "\n",
    "# bacteroides_sample_contamination = CSVTable(\"./dataset/biom/bfragilis_divisions.txt\", delimiter=\"\\t\")\n",
    "# bacteroides_sample_contamination = bacteroides_sample_contamination.load_dataframe()\n",
    "\n",
    "# sample_division = bacteroides_sample_contamination[[\"Division\", \"Sample\"]]\n",
    "# tube_ids = sample_division[\"Sample\"].map(lambda x: x.split(\"_\")[0])\n",
    "# sample_division.index = tube_ids\n",
    "# sample_division.index.name = \"tube_id\"\n",
    "\n",
    "# real_filtered = state[\"filtered_df\"]\n",
    "# real_cm = state[\"cluster_manager\"]\n",
    "# try:\n",
    "#     fake_clusters = []\n",
    "#     requested_samples = []\n",
    "#     for sample_id in state[\"filtered_df\"].index:\n",
    "#         tube_id = bacteroides_sample_info.loc[sample_id, \"tube_id\"]\n",
    "#         if str(tube_id) in sample_division.index:\n",
    "#             division = int(sample_division.loc[str(tube_id), \"Division\"]) - 1\n",
    "#             requested_samples.append(sample_id)\n",
    "#         else:\n",
    "#             division = -1\n",
    "#         fake_clusters.append(division)\n",
    "\n",
    "#     print(fake_clusters)\n",
    "#     # state[\"filtered_df\"] = state[\"filtered_df\"].loc[requested_samples]    \n",
    "#     fake_cm = ClusterManager(state[\"filtered_df\"], 1)\n",
    "        \n",
    "#     fake_cm.cluster_state = ClusterState(fake_cm, np.array(fake_clusters), {-1:1, 0:1, 1:1})\n",
    "#     state[\"cluster_manager\"] = fake_cm\n",
    "#     plot_click(None)\n",
    "# finally:\n",
    "#     pass\n",
    "#     # state[\"filtered_df\"] = real_filtered\n",
    "#     # state[\"cluster_manager\"] = real_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197dcc3-189f-45a0-8556-eaadc08bb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from detnmf.detnmf import run_detnmf\n",
    "\n",
    "# X = df.to_numpy() #np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n",
    "# model = NMF(n_components=1)\n",
    "# W = model.fit_transform(X)\n",
    "# H = model.components_\n",
    "\n",
    "# print(\"Coefficients\")\n",
    "# print(W)\n",
    "# print(\"Components\")\n",
    "# print(H)\n",
    "\n",
    "# print(np.argmax(H))\n",
    "# print(df.columns[np.argmax(H)])\n",
    "\n",
    "X = df.to_numpy()\n",
    "# X = np.array([[0, 0], [100, 100], [200, 200], [300, 300], [400, 400], [300, 100], [600, 200], [900, 300], [1200, 400], [1500, 500]])\n",
    "# model = MVCNMF(n_components=2)\n",
    "# A, S = model.fit(X, learning_rate=10)\n",
    "\n",
    "n_components = 2\n",
    "W, H = run_detnmf(X, n_components, 1000, 2500)\n",
    "\n",
    "print(\"Data\")\n",
    "print(X.shape)\n",
    "print(X.sum())\n",
    "print(X)\n",
    "print(\"Coefficients\")\n",
    "print(W)\n",
    "print(W.shape)\n",
    "print(\"Components\")\n",
    "print(H)\n",
    "print(H.shape)\n",
    "print(H.sum(axis=1))\n",
    "\n",
    "for component in range(n_components):\n",
    "    big_val = np.argmax(H[component, :])\n",
    "    print(df.columns[big_val])\n",
    "\n",
    "qq = H.copy()\n",
    "qq[0,:] = qq[0,:] / np.linalg.norm(qq[0,:])\n",
    "qq[1,:] = qq[1,:] / np.linalg.norm(qq[1,:])\n",
    "print(np.dot(qq[0,:], qq[1,:]))\n",
    "\n",
    "basis = H.T\n",
    "basis_df = pd.DataFrame(basis, index=df.columns)\n",
    "to_save = {\"all\":[basis_df.to_json()]}\n",
    "\n",
    "with open(\"./results/detnmf.json\", 'w') as outfile:\n",
    "    json.dump(to_save, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79a958-ca1a-4b84-aedb-9436331bc3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
